# å†ãƒ‡ãƒ—ãƒ­ã‚¤ç”¨ãƒ€ãƒŸãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆ
import streamlit as st
import os
import re
import pandas as pd
from datetime import datetime
import json
from google.cloud import vision
import requests
from pdf2image import convert_from_bytes
import tempfile
import platform
import io
from PyPDF2 import PdfReader
from PIL import Image
import unicodedata
import firebase_admin
from firebase_admin import credentials, firestore
import time

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    import faiss
    VECTOR_SEARCH_AVAILABLE = True
except ImportError:
    VECTOR_SEARCH_AVAILABLE = False
    st.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€sentence-transformersã€scikit-learnã€faiss-cpuã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
# HEICå¯¾å¿œï¼ˆå°†æ¥çš„ã«å¯¾å¿œäºˆå®šï¼‰
# try:
#     import pillow_heif
#     HEIC_SUPPORT = True
# except ImportError:
#     HEIC_SUPPORT = False

# OpenAI APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰å–å¾—
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")

# Streamlit Cloudã®Secretsã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
if "GOOGLE_APPLICATION_CREDENTIALS_JSON" in st.secrets:
    key_path = "/tmp/gcp_key.json"
    key_dict = json.loads(st.secrets["GOOGLE_APPLICATION_CREDENTIALS_JSON"].strip())
    with open(key_path, "w") as f:
        json.dump(key_dict, f)
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key_path

# Cloudmersive APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰å–å¾—
CLOUDMERSIVE_API_KEY = st.secrets.get("CLOUDMERSIVE_API_KEY", "")

# PDF.co APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰å–å¾—
PDFCO_API_KEY = st.secrets.get("PDFCO_API_KEY", "")

# FirebaseåˆæœŸåŒ–
def initialize_firebase():
    """Firebase Admin SDKã‚’åˆæœŸåŒ–"""
    try:
        # æ—¢ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        firebase_admin.get_app()
        return firestore.client()
    except ValueError:
        # åˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
        try:
            if "FIREBASE_SERVICE_ACCOUNT_JSON" in st.secrets:
                # Streamlit Secretsã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
                service_account_info = json.loads(st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"])
                cred = credentials.Certificate(service_account_info)
                firebase_admin.initialize_app(cred)
                return firestore.client()
            else:
                st.error("Firebaseã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return None
        except Exception as e:
            st.error(f"FirebaseåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    except Exception as e:
        st.error(f"Firebaseæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

# Firestoreã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
try:
    db = initialize_firebase()
except Exception as e:
    st.error(f"FirebaseåˆæœŸåŒ–ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    db = None

# Firebaseæ¥ç¶šã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿è¡¨ç¤ºï¼‰
if st.sidebar.checkbox('Firebaseæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’è¡¨ç¤º', value=False, key='show_firebase_debug'):
    st.write("ğŸ” Firebaseæ¥ç¶šãƒ†ã‚¹ãƒˆé–‹å§‹...")

    # Firebaseã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
    st.write("### ğŸ“Š ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªæ–¹æ³•")
    st.write("**Firebaseã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ç¢ºèªã™ã‚‹å ´åˆï¼š**")
    st.write("1. [Firebase Console](https://console.firebase.google.com/) ã«ã‚¢ã‚¯ã‚»ã‚¹")
    st.write("2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ")
    st.write("3. å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€ŒFirestore Databaseã€ã‚’ã‚¯ãƒªãƒƒã‚¯")
    st.write("4. `reviews`ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºèª")

    # Secretsã®å­˜åœ¨ç¢ºèª
    if "FIREBASE_SERVICE_ACCOUNT_JSON" in st.secrets:
        st.write("âœ… FIREBASE_SERVICE_ACCOUNT_JSON ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        try:
            # JSONã®è§£æãƒ†ã‚¹ãƒˆ
            service_account_info = json.loads(st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"])
            st.write("âœ… JSONã®è§£æã«æˆåŠŸã—ã¾ã—ãŸ")
            st.write(f"ğŸ“‹ Project ID: {service_account_info.get('project_id', 'N/A')}")
        except json.JSONDecodeError as e:
            st.error(f"âŒ JSONã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.write("ğŸ” ç¾åœ¨ã®è¨­å®šå€¤:")
            st.code(st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"][:200] + "...")
    else:
        st.error("âŒ FIREBASE_SERVICE_ACCOUNT_JSON ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # Firebaseæ¥ç¶šãƒ†ã‚¹ãƒˆ
    if db is None:
        st.error("âš ï¸ Firebaseæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚secrets.tomlã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        st.success("âœ… Firebaseæ¥ç¶šãŒç¢ºç«‹ã•ã‚Œã¾ã—ãŸã€‚")
        try:
            # ç°¡å˜ãªæ¥ç¶šãƒ†ã‚¹ãƒˆ
            test_doc = db.collection('test').document('connection_test')
            test_doc.set({'timestamp': 'test'})
            st.success("âœ… Firestoreã¸ã®æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ")
        except Exception as e:
            st.error(f"âŒ Firestoreã¸ã®æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# ãƒ•ã‚©ãƒ«ãƒ€æº–å‚™
def ensure_dirs():
    os.makedirs('input', exist_ok=True)
    os.makedirs('output', exist_ok=True)

ensure_dirs()

# HEICãƒ•ã‚¡ã‚¤ãƒ«ã‚’JPEGã«å¤‰æ›
# def convert_heic_to_jpeg(heic_path):
#     if not HEIC_SUPPORT:
#         st.error("HEICãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ã«ã¯pillow_heifãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚")
#         return None
#     try:
#         heif_file = pillow_heif.read_heif(heic_path)
#         image = Image.frombytes(
#             heif_file.mode, 
#             heif_file.size, 
#             heif_file.data,
#             "raw",
#             heif_file.mode,
#             heif_file.stride,
#         )
#         jpeg_path = heic_path.replace('.heic', '.jpg').replace('.HEIC', '.jpg')
#         image.save(jpeg_path, 'JPEG', quality=95)
#         return jpeg_path
#     except Exception as e:
#         st.error(f"HEICãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
#         return None

# Google Cloud Vision APIã§OCR
def ocr_image_gcv(image_path):
    client = vision.ImageAnnotatorClient()
    with open(image_path, "rb") as image_file:
        content = image_file.read()
    image = vision.Image(content=content)
    # type: ignore ã§linterã‚¨ãƒ©ãƒ¼ã‚’æŠ‘åˆ¶
    response = client.text_detection(image=image)  # type: ignore
    texts = response.text_annotations
    if texts:
        return texts[0].description
    return ""

def ocr_image(image_path, mode='gcv'):
    """OCRå‡¦ç†ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    if mode == 'gcv':
        return ocr_image_gcv(image_path)
    else:
        return ocr_image_gcv(image_path)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Google Cloud Vision

# ChatGPT APIã§å‹˜å®šç§‘ç›®ã‚’æ¨æ¸¬
def guess_account_ai(text, stance='received', extra_prompt=''):
    """å¾“æ¥ã®AIæ¨æ¸¬ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
    # å­¦ç¿’æ©Ÿèƒ½ã®ON/OFFã‚’ãƒã‚§ãƒƒã‚¯
    learning_enabled = st.session_state.get('learning_enabled', True)
    if learning_enabled:
        return guess_account_ai_with_learning(text, stance, extra_prompt)
    else:
        # å­¦ç¿’æ©Ÿèƒ½ãŒç„¡åŠ¹ã®å ´åˆã¯å¾“æ¥ã®æ–¹æ³•
        return guess_account_ai_basic(text, stance, extra_prompt)

def guess_account_ai_basic(text, stance='received', extra_prompt=''):
    """å­¦ç¿’æ©Ÿèƒ½ãªã—ã®åŸºæœ¬çš„ãªAIæ¨æ¸¬"""
    if not OPENAI_API_KEY:
        st.warning("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIæ¨æ¸¬ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        return None
    if stance == 'issued':
        stance_prompt = "ã‚ãªãŸã¯è«‹æ±‚æ›¸ã‚’ç™ºè¡Œã—ãŸå´ï¼ˆå£²ä¸Šè¨ˆä¸Šå´ï¼‰ã®çµŒç†æ‹…å½“è€…ã§ã™ã€‚å£²ä¸Šãƒ»åå…¥ã«è©²å½“ã™ã‚‹å‹˜å®šç§‘ç›®ã®ã¿ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
        account_list = "å£²ä¸Šé«˜ã€é›‘åå…¥ã€å—å–æ‰‹å½¢ã€å£²æ›é‡‘"
    else:
        stance_prompt = "ã‚ãªãŸã¯è«‹æ±‚æ›¸ã‚’å—é ˜ã—ãŸå´ï¼ˆè²»ç”¨è¨ˆä¸Šå´ï¼‰ã®çµŒç†æ‹…å½“è€…ã§ã™ã€‚è²»ç”¨ãƒ»ä»•å…¥ãƒ»è²©ç®¡è²»ã«è©²å½“ã™ã‚‹å‹˜å®šç§‘ç›®ã®ã¿ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
        account_list = "ç ”ä¿®è²»ã€æ•™è‚²ç ”ä¿®è²»ã€æ—…è²»äº¤é€šè²»ã€é€šä¿¡è²»ã€æ¶ˆè€—å“è²»ã€ä¼šè­°è²»ã€äº¤éš›è²»ã€åºƒå‘Šå®£ä¼è²»ã€å¤–æ³¨è²»ã€æ”¯æ‰•æ‰‹æ•°æ–™ã€ä»®æ‰•é‡‘ã€ä¿®ç¹•è²»ã€ä»•å…¥é«˜ã€æ¸›ä¾¡å„Ÿå´è²»"
    prompt = (
        f"{stance_prompt}\n"
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯é ˜åæ›¸ã‚„è«‹æ±‚æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚\n"
        f"å¿…ãšä¸‹è¨˜ã®å‹˜å®šç§‘ç›®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã‚’1ã¤ã ã‘æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "\nã€å‹˜å®šç§‘ç›®ãƒªã‚¹ãƒˆã€‘\n{account_list}\n"
        "\næ‘˜è¦ã‚„å•†å“åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åãƒ»è¬›ç¾©åã‚’ãã®ã¾ã¾å‹˜å®šç§‘ç›®ã«ã—ãªã„ã§ãã ã•ã„ã€‚\n"
        "ãŸã¨ãˆã°ã€SNSè¬›ç¾©è²»ã€ã‚„ã€â—‹â—‹ã‚»ãƒŸãƒŠãƒ¼è²»ã€ãªã©ã¯ã€ç ”ä¿®è²»ã€ã‚„ã€æ•™è‚²ç ”ä¿®è²»ã€ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n"
        "åˆ†ã‹ã‚‰ãªã„å ´åˆã¯å¿…ãšã€ä»®æ‰•é‡‘ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "\nâ€»ã€ãƒ¬ã‚¿ãƒ¼ãƒ‘ãƒƒã‚¯ã€ã€åˆ‡æ‰‹ã€ã€éƒµä¾¿ã€ã€ã‚†ã†ãƒ‘ãƒƒã‚¯ã€ã€ã‚†ã†ãƒ¡ãƒ¼ãƒ«ã€ã€ã‚†ã†ãƒ‘ã‚±ãƒƒãƒˆã€ã€ã‚¹ãƒãƒ¼ãƒˆãƒ¬ã‚¿ãƒ¼ã€ã€ãƒŸãƒ‹ãƒ¬ã‚¿ãƒ¼ã€ãªã©éƒµä¾¿ãƒ»é…é€ã‚µãƒ¼ãƒ“ã‚¹ã«è©²å½“ã™ã‚‹å ´åˆã¯å¿…ãšã€é€šä¿¡è²»ã€ã¨ã—ã¦ãã ã•ã„ã€‚\n"
        "â€»ã€é£²æ–™ã€ã€é£Ÿå“ã€ã€ãŠè“å­ã€ã€ãƒšãƒƒãƒˆãƒœãƒˆãƒ«ã€ã€å¼å½“ã€ã€ãƒ‘ãƒ³ã€ã€ã‚³ãƒ¼ãƒ’ãƒ¼ã€ã€ãŠèŒ¶ã€ã€æ°´ã€ã€ã‚¸ãƒ¥ãƒ¼ã‚¹ã€ãªã©é£²é£Ÿç‰©ã‚„è»½é£Ÿãƒ»ä¼šè­°ç”¨ã®é£Ÿã¹ç‰©ãƒ»é£²ã¿ç‰©ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã€ä¼šè­°è²»ã¾ãŸã¯æ¶ˆè€—å“è²»ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n"
        "\nã€è‰¯ã„ä¾‹ã€‘\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: SNSè¬›ç¾©è²» 10,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šç ”ä¿®è²»\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒ¬ã‚¿ãƒ¼ãƒ‘ãƒƒã‚¯ãƒ—ãƒ©ã‚¹ 1,200å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šé€šä¿¡è²»\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒšãƒƒãƒˆãƒœãƒˆãƒ«é£²æ–™ãƒ»ãŠè“å­ 2,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šä¼šè­°è²»\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: é£Ÿå“ãƒ»é£²æ–™ãƒ»ãƒ‘ãƒ³ 1,500å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šæ¶ˆè€—å“è²»\n"
        "\nã€æ‚ªã„ä¾‹ã€‘\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: SNSè¬›ç¾©è²» 10,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šSNSè¬›ç¾©è²»ï¼ˆÃ—ï¼‰\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒ¬ã‚¿ãƒ¼ãƒ‘ãƒƒã‚¯ãƒ—ãƒ©ã‚¹ 1,200å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šåºƒå‘Šå®£ä¼è²»ï¼ˆÃ—ï¼‰\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒšãƒƒãƒˆãƒœãƒˆãƒ«é£²æ–™ãƒ»ãŠè“å­ 2,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šé€šä¿¡è²»ï¼ˆÃ—ï¼‰\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: é£Ÿå“ãƒ»é£²æ–™ãƒ»ãƒ‘ãƒ³ 1,500å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šé€šä¿¡è²»ï¼ˆÃ—ï¼‰\n"
        f"\nã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\nå‹˜å®šç§‘ç›®ï¼š"
    ) + (f"\nã€è¿½åŠ æŒ‡ç¤ºã€‘\n{extra_prompt}" if extra_prompt else "")
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-4.1-nano",
        "messages": [
            {"role": "system", "content": stance_prompt},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 20,
        "temperature": 0
    }
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        account = content.split("\n")[0].replace("å‹˜å®šç§‘ç›®ï¼š", "").strip()
        return account
    except Exception as e:
        st.warning(f"AIã«ã‚ˆã‚‹å‹˜å®šç§‘ç›®æ¨æ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# æ‘˜è¦ã‚’AIã§ç”Ÿæˆ

def guess_description_ai(text, period_hint=None, extra_prompt=''):
    if not OPENAI_API_KEY:
        return ""
    period_instruction = ""
    if period_hint:
        period_instruction = f"\nã“ã®è«‹æ±‚æ›¸ã«ã¯ã€{period_hint}ã€ã¨ã„ã†æœŸé–“æƒ…å ±ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚æ‘˜è¦ã«ã¯å¿…ãšã“ã®æƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"
    prompt = (
        "ã‚ãªãŸã¯æ—¥æœ¬ã®ä¼šè¨ˆå®Ÿå‹™ã«è©³ã—ã„çµŒç†æ‹…å½“è€…ã§ã™ã€‚\n"
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯é ˜åæ›¸ã‚„è«‹æ±‚æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚\n"
        "æ‘˜è¦æ¬„ã«ã¯ã€ä½•ã«ä½¿ã£ãŸã‹ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åãƒ»è¬›ç¾©åãªã©ã€é ˜åæ›¸ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å…·ä½“çš„ãªç”¨é€”ã‚„å†…å®¹ã‚’20æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«æ—¥æœ¬èªã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\n"
        "é‡‘é¡ã‚„ã€æ¶ˆè²»ç¨ã€ãªã©ã®å˜èªã ã‘ã‚’æ‘˜è¦ã«ã—ãªã„ã§ãã ã•ã„ã€‚\n"
        "ã¾ãŸã€ã€xæœˆåˆ†ã€ã€ä¸ŠæœŸåˆ†ã€ã€ä¸‹æœŸåˆ†ã€ãªã©ã®æœŸé–“æƒ…å ±ãŒã‚ã‚Œã°å¿…ãšæ‘˜è¦ã«å«ã‚ã¦ãã ã•ã„ã€‚"
        f"{period_instruction}"
        "\nã€è‰¯ã„ä¾‹ã€‘\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: 4æœˆåˆ†PRå ±é…¬ äº¤é€šè²» 1,000å†† ã‚¿ã‚¯ã‚·ãƒ¼åˆ©ç”¨\nâ†’ æ‘˜è¦ï¼š4æœˆåˆ†PRå ±é…¬ ã‚¿ã‚¯ã‚·ãƒ¼ç§»å‹•\n"
        "\nã€æ‚ªã„ä¾‹ã€‘\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: 4æœˆåˆ†PRå ±é…¬ äº¤é€šè²» 1,000å†† ã‚¿ã‚¯ã‚·ãƒ¼åˆ©ç”¨\nâ†’ æ‘˜è¦ï¼š1,000å††ï¼ˆÃ—ï¼‰\n"
        f"\nã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\næ‘˜è¦ï¼š"
    ) + (f"\nã€è¿½åŠ æŒ‡ç¤ºã€‘\n{extra_prompt}" if extra_prompt else "")
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-4.1-nano",
        "messages": [
            {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬ã®ä¼šè¨ˆä»•è¨³ã«è©³ã—ã„çµŒç†æ‹…å½“è€…ã§ã™ã€‚æ‘˜è¦æ¬„ã«ã¯ç”¨é€”ã‚„å†…å®¹ãŒåˆ†ã‹ã‚‹æ—¥æœ¬èªã‚’ç°¡æ½”ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 40,
        "temperature": 0
    }
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        description = content.split("\n")[0].replace("æ‘˜è¦ï¼š", "").strip()
        return description
    except Exception:
        return ""

# é‡‘é¡ã‚’AIã§æŠ½å‡º

def guess_amount_ai(text):
    if not OPENAI_API_KEY:
        return None
    prompt = (
        "ä»¥ä¸‹ã¯æ—¥æœ¬ã®è«‹æ±‚æ›¸ã‚„é ˜åæ›¸ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚"
        "ã“ã®è«‹æ±‚æ›¸ã®åˆè¨ˆé‡‘é¡ï¼ˆæ”¯æ‰•é‡‘é¡ã€ç¨è¾¼ï¼‰ã‚’æ•°å­—ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        "çµ¶å¯¾ã«å£åº§ç•ªå·ãƒ»ç™»éŒ²ç•ªå·ãƒ»é›»è©±ç•ªå·ãƒ»æŒ¯è¾¼å…ˆãƒ»é€£çµ¡å…ˆãƒ»ç™»éŒ²ç•ªå·ãƒ»TELãƒ»No.ãªã©ã®æ•°å­—ã‚„ã€10æ¡ä»¥ä¸Šã®æ•°å­—ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ãªã„é•·ã„æ•°å­—ã¯é‡‘é¡ã¨ã—ã¦å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚"
        "åˆè¨ˆé‡‘é¡ã¯ã€åˆè¨ˆã€ã€å°è¨ˆã€ã€ã”è«‹æ±‚é‡‘é¡ã€ã€è«‹æ±‚é‡‘é¡ã€ã€ç·é¡ã€ã€ç¾é‡‘æ”¯æ‰•é¡ã€ãªã©ã®ãƒ©ãƒ™ãƒ«ã®ç›´å¾Œã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚"
        "é‡‘é¡ã®ã‚«ãƒ³ãƒã‚„ã‚¹ãƒšãƒ¼ã‚¹ã€æ”¹è¡ŒãŒæ··ã˜ã£ã¦ã„ã¦ã‚‚æ­£ã—ã„åˆè¨ˆé‡‘é¡ï¼ˆä¾‹ï¼š1,140å††ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"
        "ã€ãŠé ã‚Šã€ã€ãŠé ã‹ã‚Šã€ã€ãŠé‡£ã‚Šã€ã€ç¾é‡‘ã€ãªã©ã®ãƒ©ãƒ™ãƒ«ãŒä»˜ã„ãŸé‡‘é¡ã¯çµ¶å¯¾ã«é¸ã°ãªã„ã§ãã ã•ã„ã€‚"
        "è¤‡æ•°ã®é‡‘é¡ãŒã‚ã‚‹å ´åˆã¯ã€åˆè¨ˆãƒ»ç·é¡ãªã©ã®ãƒ©ãƒ™ãƒ«ä»˜ãã§æœ€ã‚‚ä¸‹ã«ã‚ã‚‹ã‚‚ã®ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
        "åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ¬„ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        "ã€è‰¯ã„ä¾‹ã€‘\nãƒ†ã‚­ã‚¹ãƒˆ: åˆè¨ˆ Â¥1, 140\nâ†’ 1140\nãƒ†ã‚­ã‚¹ãƒˆ: åˆè¨ˆ 18,000å†† æŒ¯è¾¼å…ˆ: 2688210\nâ†’ 18000\nã€æ‚ªã„ä¾‹ã€‘\nãƒ†ã‚­ã‚¹ãƒˆ: åˆè¨ˆ Â¥1, 140\nâ†’ 1ï¼ˆÃ—ï¼‰ã‚„140ï¼ˆÃ—ï¼‰\nãƒ†ã‚­ã‚¹ãƒˆ: åˆè¨ˆ 18,000å†† æŒ¯è¾¼å…ˆ: 2688210\nâ†’ 2688210ï¼ˆÃ—ï¼‰"
        "\n\nãƒ†ã‚­ã‚¹ãƒˆ:\n{text}\n\nåˆè¨ˆé‡‘é¡ï¼š"
    )
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-4.1-nano",
        "messages": [
            {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬ã®ä¼šè¨ˆå®Ÿå‹™ã«è©³ã—ã„çµŒç†æ‹…å½“è€…ã§ã™ã€‚è«‹æ±‚æ›¸ã‚„é ˜åæ›¸ã‹ã‚‰åˆè¨ˆé‡‘é¡ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 20,
        "temperature": 0
    }
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        amount_str = content.split("\n")[0].replace("åˆè¨ˆé‡‘é¡ï¼š", "").replace(",", "").replace(" ", "").strip()
        if amount_str.isdigit():
            return int(amount_str)
        return None
    except Exception as e:
        st.warning(f"AIã«ã‚ˆã‚‹é‡‘é¡æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# å¹´åº¦è¡¨è¨˜ã‚’é™¤å¤–ã™ã‚‹é–¢æ•°
def is_year_number(val, text):
    """å¹´åº¦è¡¨è¨˜ï¼ˆ2025ãªã©ï¼‰ã‚’é™¤å¤–ã™ã‚‹"""
    if val >= 2020 and val <= 2030:  # å¹´åº¦ã®ç¯„å›²
        # å¹´åº¦è¡¨è¨˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        year_patterns = [
            r'\d{4}å¹´',
            r'\d{4}/',
            r'\d{4}-',
            r'å¹´åº¦',
            r'FY\d{4}',
            r'fiscal.*\d{4}'
        ]
        for pattern in year_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
    return False

def preprocess_receipt_text(text):
    # å…¨è§’â†’åŠè§’å¤‰æ›ã€ä½™è¨ˆãªæ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ã€é‡‘é¡åŒºåˆ‡ã‚Šè¨˜å·ã€Œ.ã€â†’ã€Œ,ã€
    import re
    text = unicodedata.normalize('NFKC', text)
    text = text.replace('\r', '')
    text = text.replace('.', ',')  # é‡‘é¡åŒºåˆ‡ã‚Šè¨˜å·ã‚’ã‚«ãƒ³ãƒã«çµ±ä¸€
    # é‡‘é¡éƒ¨åˆ†ã®ã‚«ãƒ³ãƒãƒ»ã‚¹ãƒšãƒ¼ã‚¹æ··åœ¨ï¼ˆä¾‹ï¼šÂ¥1, 140ï¼‰ã‚’Â¥1,140ã«æ­£è¦åŒ–
    text = re.sub(r'Â¥([0-9]+),\s*([0-9]{3})', r'Â¥\1,\2', text)
    text = re.sub(r'Â¥([0-9]+)\s*,\s*([0-9]{3})', r'Â¥\1,\2', text)
    text = re.sub(r'Â¥([0-9]+)\s+([0-9]{3})', r'Â¥\1,\2', text)
    # æ‹¬å¼§å†…ã®å¤–8%/å¤–10%ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¤‡æ•°è¡Œã«ã¾ãŸãŒã£ã¦ã‚‚1è¡Œã«é€£çµ
    def merge_parentheses_lines(txt):
        lines = txt.split('\n')
        merged = []
        buf = []
        inside = False
        for line in lines:
            if re.match(r'^\(å¤–\s*[810]{1,2}[%ï¼…]', line):
                inside = True
                buf.append(line)
            elif inside:
                buf.append(line)
                if ')' in line:
                    merged.append(' '.join(buf))
                    buf = []
                    inside = False
            else:
                merged.append(line)
        if buf:
            merged.append(' '.join(buf))
        return '\n'.join(merged)
    text = merge_parentheses_lines(text)
    text = '\n'.join([line.strip() for line in text.split('\n') if line.strip()])
    return text

# é‡‘é¡ãƒ»ç¨ç‡ã”ã¨ã®è¤‡æ•°ä»•è¨³ç”Ÿæˆé–¢æ•°
def extract_multiple_entries(text, stance='received', tax_mode='è‡ªå‹•åˆ¤å®š', debug_mode=False, extra_prompt=''):
    """10%ãƒ»8%æ··åœ¨ãƒ¬ã‚·ãƒ¼ãƒˆã«å¯¾å¿œã—ãŸè¤‡æ•°ä»•è¨³ç”Ÿæˆï¼ˆå …ç‰¢ãªæ­£è¦è¡¨ç¾ãƒ»ç¨ç‡ã”ã¨ã®å†…ç¨/å¤–ç¨åˆ¤å®šãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–ï¼‰"""
    text = preprocess_receipt_text(text)
    entries = []
    
    # --- ãƒ‡ãƒãƒƒã‚°å¼·åŒ–: å…¨è¡Œã®å†…å®¹ã¨ãƒ’ãƒƒãƒˆçŠ¶æ³ã‚’å¿…ãšè¡¨ç¤ºï¼ˆæœ€åˆã«å®Ÿè¡Œï¼‰ ---
    tax_blocks = []
    debug_lines = []
    lines = text.split('\n')
    for i, line in enumerate(lines):
        hit = []
        # èª²ç¨10%
        m10 = re.search(r'èª²ç¨è¨ˆ\s*[\(ï¼ˆ]10[%ï¼…][\)ï¼‰]', line)
        if m10:
            hit.append('èª²ç¨10%ãƒ©ãƒ™ãƒ«')
            # æ¬¡è¡Œã«é‡‘é¡ãŒã‚ã‚Œã°æŠ½å‡º
            if i+1 < len(lines):
                mval = re.search(r'Â¥?([0-9,]+)', lines[i+1])
                if mval:
                    val = int(mval.group(1).replace(',', ''))
                    # é‡‘é¡è¡Œã«ã€ŒÂ¥ã€ã‚„ã€Œå††ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã¤2å††ä»¥ä¸Šï¼ˆ0å††ãƒ»1å††ã¯é™¤å¤–ï¼‰
                    if (('Â¥' in lines[i+1] or 'å††' in lines[i+1]) and val > 1):
                        tax_blocks.append(('å¤–ç¨10%', val, 'èª²ç¨ä»•å…¥ 10%', line + ' / ' + lines[i+1]))
                        hit.append(f'é‡‘é¡:{val}')
        # èª²ç¨8%
        m8 = re.search(r'èª²ç¨è¨ˆ\s*[\(ï¼ˆ]8[%ï¼…][\)ï¼‰]', line)
        if m8:
            hit.append('èª²ç¨8%ãƒ©ãƒ™ãƒ«')
            if i+1 < len(lines):
                mval = re.search(r'Â¥?([0-9,]+)', lines[i+1])
                if mval:
                    val = int(mval.group(1).replace(',', ''))
                    if (('Â¥' in lines[i+1] or 'å††' in lines[i+1]) and val > 1):
                        tax_blocks.append(('å¤–ç¨8%', val, 'èª²ç¨ä»•å…¥ 8%', line + ' / ' + lines[i+1]))
                        hit.append(f'é‡‘é¡:{val}')
        # éèª²ç¨
        mex = re.search(r'éèª²[ç¨ç¨…]è¨ˆ', line)
        if mex:
            hit.append('éèª²ç¨ãƒ©ãƒ™ãƒ«')
            if i+1 < len(lines):
                mval = re.search(r'Â¥?([0-9,]+)', lines[i+1])
                if mval:
                    val = int(mval.group(1).replace(',', ''))
                    if (('Â¥' in lines[i+1] or 'å††' in lines[i+1]) and val > 1):
                        tax_blocks.append(('éèª²ç¨', val, 'éèª²ç¨', line + ' / ' + lines[i+1]))
                        hit.append(f'é‡‘é¡:{val}')
        debug_lines.append(f'[{i}] {line} => {hit if hit else "ãƒ’ãƒƒãƒˆãªã—"}')
    # ãƒ‡ãƒãƒƒã‚°ç”¨: Streamlitã§å…¨è¡Œã®ãƒ’ãƒƒãƒˆçŠ¶æ³ã‚’å¿…ãšè¡¨ç¤º
    if debug_mode and 'st' in globals():
        st.info("[ãƒ‡ãƒãƒƒã‚°] å„è¡Œã®æ­£è¦è¡¨ç¾ãƒ’ãƒƒãƒˆçŠ¶æ³:\n" + '\n'.join(debug_lines))
        if tax_blocks:
            st.info(f"[ãƒ‡ãƒãƒƒã‚°] ç¨åŒºåˆ†ãƒ»é‡‘é¡ãƒšã‚¢æŠ½å‡ºçµæœ: {[(mode, val, label, l) for mode, val, label, l in tax_blocks]}")
        else:
            st.info("[ãƒ‡ãƒãƒƒã‚°] ç¨åŒºåˆ†ãƒ»é‡‘é¡ãƒšã‚¢æŠ½å‡ºçµæœ: ãªã—")
    # --- ã“ã“ã¾ã§ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ï¼ˆæœ€åˆã«å®Ÿè¡Œï¼‰ ---
    
    # ãƒ‡ãƒãƒƒã‚°ã§æŠ½å‡ºã•ã‚ŒãŸç¨åŒºåˆ†ãƒ»é‡‘é¡ãƒšã‚¢ãŒã‚ã‚Œã°ä½¿ç”¨
    if tax_blocks:
        for mode, amount, tax_label, _ in tax_blocks:
            entry = extract_info_from_text(text, stance, mode, extra_prompt=extra_prompt)
            entry['amount'] = str(amount)
            if mode == 'éèª²ç¨':
                entry['tax'] = '0'
            entry['description'] = f"{entry['description']}ï¼ˆ{tax_label}ï¼‰"
            entries.append(entry)
        return entries
    
    # (å¤–8% å¯¾è±¡ Â¥962)ã‚„(å¤–10% å¯¾è±¡ Â¥420)ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºï¼ˆè¤‡æ•°è¡Œå¯¾å¿œãƒ»findallã§å…¨ã¦æŠ½å‡ºï¼‰
    pattern_8 = re.compile(r'å¤–\s*8[%ï¼…][^\d\n]*?å¯¾è±¡[^\d\n]*?Â¥?([0-9,]+)', re.IGNORECASE | re.DOTALL)
    pattern_10 = re.compile(r'å¤–\s*10[%ï¼…][^\d\n]*?å¯¾è±¡[^\d\n]*?Â¥?([0-9,]+)', re.IGNORECASE | re.DOTALL)
    amounts_8 = [int(m.replace(',', '')) for m in pattern_8.findall(text) if m and int(m.replace(',', '')) > 10]
    amounts_10 = [int(m.replace(',', '')) for m in pattern_10.findall(text) if m and int(m.replace(',', '')) > 10]
    # 8%ä»•è¨³
    for amount_8 in amounts_8:
        entry_8 = extract_info_from_text(text, stance, 'å¤–ç¨8%', extra_prompt=extra_prompt)
        entry_8['amount'] = str(amount_8)
        entry_8['tax'] = str(int(amount_8 * 0.08))
        entry_8['description'] = f"{entry_8['description']}ï¼ˆ8%å¯¾è±¡ï¼‰"
        entries.append(entry_8)
    # 10%ä»•è¨³
    for amount_10 in amounts_10:
        entry_10 = extract_info_from_text(text, stance, 'å¤–ç¨10%', extra_prompt=extra_prompt)
        entry_10['amount'] = str(amount_10)
        entry_10['tax'] = str(int(amount_10 * 0.1))
        entry_10['description'] = f"{entry_10['description']}ï¼ˆ10%å¯¾è±¡ï¼‰"
        entries.append(entry_10)
    if entries:
        return entries
    # è¤‡æ•°è¡Œã«ã¾ãŸãŒã‚‹ã€Œå†…8%ã€ã€Œå†…10%ã€ã®å°è¨ˆãƒ»ç¨é¡æŠ½å‡º
    # ä¾‹ï¼š(å†… 8% ã‚¿ã‚¤ã‚·ãƒ§ã‚¦\nÂ¥1,755)  (å†… 8%\nÂ¥130)
    pattern_8 = re.compile(r'å†…\s*8[%ï¼…][^\d\n]*[\(ï¼ˆ\[ï½¢]?(?:ã‚¿ã‚¤ã‚·ãƒ§ã‚¦)?[\sã€€]*\n?Â¥?([0-9,]+)[\)ï¼‰\]ï½£]?', re.IGNORECASE)
    pattern_8_tax = re.compile(r'å†…\s*8[%ï¼…][^\d\n]*\n?Â¥?([0-9,]+)[\)ï¼‰\]ï½£]?', re.IGNORECASE)
    pattern_10 = re.compile(r'å†…\s*10[%ï¼…][^\d\n]*[\(ï¼ˆ\[ï½¢]?(?:ã‚¿ã‚¤ã‚·ãƒ§ã‚¦)?[\sã€€]*\n?Â¥?([0-9,]+)[\)ï¼‰\]ï½£]?', re.IGNORECASE)
    pattern_10_tax = re.compile(r'å†…\s*10[%ï¼…][^\d\n]*\n?Â¥?([0-9,]+)[\)ï¼‰\]ï½£]?', re.IGNORECASE)
    # å°è¨ˆ
    match_8 = pattern_8.search(text)
    match_10 = pattern_10.search(text)
    # ç¨é¡
    matches_8_tax = pattern_8_tax.findall(text)
    matches_10_tax = pattern_10_tax.findall(text)
    amount_8 = int(match_8.group(1).replace(',', '')) if match_8 and match_8.group(1) else None
    amount_10 = int(match_10.group(1).replace(',', '')) if match_10 and match_10.group(1) else None
    # ç¨é¡ã¯2å›ç›®ã®å‡ºç¾ã‚’å„ªå…ˆï¼ˆ1å›ç›®ã¯å°è¨ˆã€2å›ç›®ã¯ç¨é¡ã§ã‚ã‚‹ã“ã¨ãŒå¤šã„ï¼‰
    tax_8 = int(matches_8_tax[1].replace(',', '')) if len(matches_8_tax) > 1 else None
    tax_10 = int(matches_10_tax[1].replace(',', '')) if len(matches_10_tax) > 1 else None
    # ã€Œå†…8%ã€ã€Œå†…10%ã€ãŒå‡ºç¾ã—ãŸå ´åˆã¯å†…ç¨ã¨ã—ã¦æ‰±ã†
    mode_8 = 'å†…ç¨' if 'å†…8%' in text or 'å†… 8%' in text else 'å¤–ç¨'
    mode_10 = 'å†…ç¨' if 'å†…10%' in text or 'å†… 10%' in text else 'å¤–ç¨'
    # 8%ä»•è¨³
    if amount_8 and amount_8 > 10:
        entry_8 = extract_info_from_text(text, stance, f'{mode_8}8%', extra_prompt=extra_prompt)
        entry_8['amount'] = str(amount_8)
        entry_8['tax'] = str(tax_8 if tax_8 is not None else (amount_8 - int(round(amount_8 / 1.08)) if mode_8 == 'å†…ç¨' else int(amount_8 * 0.08)))
        entry_8['description'] = f"{entry_8['description']}ï¼ˆ8%å¯¾è±¡ï¼‰"
        entries.append(entry_8)
    # 10%ä»•è¨³
    if amount_10 and amount_10 > 10:
        entry_10 = extract_info_from_text(text, stance, f'{mode_10}10%', extra_prompt=extra_prompt)
        entry_10['amount'] = str(amount_10)
        entry_10['tax'] = str(tax_10 if tax_10 is not None else (amount_10 - int(round(amount_10 / 1.1)) if mode_10 == 'å†…ç¨' else int(amount_10 * 0.1)))
        entry_10['description'] = f"{entry_10['description']}ï¼ˆ10%å¯¾è±¡ï¼‰"
        entries.append(entry_10)
    if entries:
        return entries
    # æ˜ç´°è¡Œãƒ™ãƒ¼ã‚¹ã®æ··åœ¨åˆ¤å®šï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    # ãƒ¬ã‚·ãƒ¼ãƒˆä¸‹éƒ¨ã®å†…8%ãƒ»å†…10%é‡‘é¡ãƒ»ç¨é¡æŠ½å‡º
    # ä¾‹: å†…8%ï¼ˆ\708ï¼‰(ç¨é¡\52)  å†…10%ï¼ˆ\130ï¼‰(ç¨é¡\12)
    bottom_8 = re.search(r'å†…[\sã€€]*8[%ï¼…][^\d]*(?:\\?([0-9,]+))[^\d]*(?:ç¨é¡[\sã€€]*\\?([0-9,]+))?', text)
    bottom_10 = re.search(r'å†…[\sã€€]*10[%ï¼…][^\d]*(?:\\?([0-9,]+))[^\d]*(?:ç¨é¡[\sã€€]*\\?([0-9,]+))?', text)
    amount_8 = int(bottom_8.group(1).replace(',', '')) if bottom_8 and bottom_8.group(1) else None
    tax_8 = int(bottom_8.group(2).replace(',', '')) if bottom_8 and bottom_8.group(2) else None
    amount_10 = int(bottom_10.group(1).replace(',', '')) if bottom_10 and bottom_10.group(1) else None
    tax_10 = int(bottom_10.group(2).replace(',', '')) if bottom_10 and bottom_10.group(2) else None
    # å†…ç¨/å¤–ç¨åˆ¤å®š
    is_inclusive = bool(re.search(r'å†…ç¨|ç¨è¾¼|æ¶ˆè²»ç¨è¾¼|tax in|tax-in|taxin', text.lower()))
    is_exclusive = bool(re.search(r'å¤–ç¨|åˆ¥é€”æ¶ˆè²»ç¨|tax out|tax-out|taxout', text.lower()))
    # 10%ãƒ»8%æ··åœ¨ã®åˆ¤å®šï¼ˆæ˜ç´°è¡Œã‚‚å«ã‚€ï¼‰
    has_10_percent = re.search(r'10%|ï¼‘ï¼ï¼…|æ¶ˆè²»ç¨.*10|ç¨ç‡.*10', text)
    has_8_percent = re.search(r'8%|ï¼˜ï¼…|æ¶ˆè²»ç¨.*8|ç¨ç‡.*8', text)
    # æ˜ç´°è¡Œã‹ã‚‰é‡‘é¡ãƒ»ç¨ç‡ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ã‚‚æ®‹ã™ï¼‰
    lines = text.split('\n')
    item_amounts = []
    for line in lines:
        if re.search(r'([0-9,]+)å††.*[0-9]+%|([0-9,]+)å††.*ï¼˜ï¼…|([0-9,]+)å††.*10%', line):
            amount_match = re.search(r'([0-9,]+)å††', line)
            if amount_match:
                amount = int(amount_match.group(1).replace(',', ''))
                if re.search(r'8%|ï¼˜ï¼…', line):
                    tax_rate = 8
                elif re.search(r'10%|ï¼‘ï¼ï¼…', line):
                    tax_rate = 10
                else:
                    tax_rate = 10
                item_amounts.append({'amount': amount, 'tax_rate': tax_rate, 'line': line})
    # ãƒ¬ã‚·ãƒ¼ãƒˆä¸‹éƒ¨ã®é‡‘é¡ãŒã‚ã‚Œã°å„ªå…ˆ
    if amount_8 or amount_10:
        if amount_10:
            entry_10 = extract_info_from_text(text, stance, 'å†…ç¨10%' if is_inclusive else 'å¤–ç¨10%', extra_prompt=extra_prompt)
            entry_10['amount'] = str(amount_10)
            entry_10['tax'] = str(tax_10 if tax_10 is not None else (amount_10 - int(round(amount_10 / 1.1)) if is_inclusive else int(amount_10 * 0.1)))
            entry_10['description'] = f"{entry_10['description']}ï¼ˆ10%å¯¾è±¡ï¼‰"
            entries.append(entry_10)
        if amount_8:
            entry_8 = extract_info_from_text(text, stance, 'å†…ç¨8%' if is_inclusive else 'å¤–ç¨8%', extra_prompt=extra_prompt)
            entry_8['amount'] = str(amount_8)
            entry_8['tax'] = str(tax_8 if tax_8 is not None else (amount_8 - int(round(amount_8 / 1.08)) if is_inclusive else int(amount_8 * 0.08)))
            entry_8['description'] = f"{entry_8['description']}ï¼ˆ8%å¯¾è±¡ï¼‰"
            entries.append(entry_8)
        return entries
    # æ˜ç´°è¡Œãƒ™ãƒ¼ã‚¹ã®æ··åœ¨åˆ¤å®š
    if has_10_percent and has_8_percent and len(item_amounts) > 1:
        amounts_10 = [item for item in item_amounts if item['tax_rate'] == 10]
        amounts_8 = [item for item in item_amounts if item['tax_rate'] == 8]
        if amounts_10:
            total_10 = sum(item['amount'] for item in amounts_10)
            entry_10 = extract_info_from_text(text, stance, 'å†…ç¨10%' if is_inclusive else 'å¤–ç¨10%', extra_prompt=extra_prompt)
            entry_10['amount'] = str(total_10)
            entry_10['tax'] = str(int(total_10 * 0.1))
            entry_10['description'] = f"{entry_10['description']}ï¼ˆ10%å¯¾è±¡ï¼‰"
            entries.append(entry_10)
        if amounts_8:
            total_8 = sum(item['amount'] for item in amounts_8)
            entry_8 = extract_info_from_text(text, stance, 'å†…ç¨8%' if is_inclusive else 'å¤–ç¨8%', extra_prompt=extra_prompt)
            entry_8['amount'] = str(total_8)
            entry_8['tax'] = str(int(total_8 * 0.08))
            entry_8['description'] = f"{entry_8['description']}ï¼ˆ8%å¯¾è±¡ï¼‰"
            entries.append(entry_8)
        return entries
    # å˜ä¸€ç¨ç‡ã¾ãŸã¯æ··åœ¨ã§ãªã„å ´åˆ
    entry = extract_info_from_text(text, stance, tax_mode, extra_prompt=extra_prompt)
    entries.append(entry)
    return entries

# ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆé‡‘é¡æŠ½å‡ºç²¾åº¦å¼·åŒ–ç‰ˆï¼‰
def extract_info_from_text(text, stance='received', tax_mode='è‡ªå‹•åˆ¤å®š', extra_prompt=''):
    info = {
        'company': '',
        'date': '',
        'amount': '',
        'tax': '',
        'description': '',
        'account': '',
        'account_source': '',
        'ocr_text': text
    }
    lines = text.split('\n')
    for line in lines:
        if any(keyword in line for keyword in ['æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'Studio', 'Inc', 'Corp']):
            company_line = line.strip()
            # ä½™è¨ˆãªæœŸé–“æƒ…å ±ãªã©ã‚’é™¤å»
            company_line = re.sub(r'(é›†è¨ˆæœŸé–“|æœŸé–“|\d{1,2}æœˆåˆ†|[0-9]{4}/[0-9]{2}/[0-9]{2}ï½[0-9]{4}/[0-9]{2}/[0-9]{2}|[0-9]{4}å¹´[0-9]{1,2}æœˆåˆ†).*?(æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆåŒä¼šç¤¾|Studio|Inc|Corp)', r'\2', company_line)
            # ä¼šç¤¾åéƒ¨åˆ†ã ã‘æŠ½å‡º
            match = re.search(r'(æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆåŒä¼šç¤¾|Studio|Inc|Corp)[^\s]*.*', company_line)
            if match:
                company_name = match.group(0)
            else:
                company_name = company_line
            # æ•¬ç§°ã‚’é™¤å»
            for suffix in ['å¾¡ä¸­', 'æ§˜', 'æ®¿', 'ã•ã‚“', 'å›', 'ã¡ã‚ƒã‚“']:
                if company_name.endswith(suffix):
                    company_name = company_name[:-len(suffix)]
                    break
            # æ³•äººç¨®åˆ¥ã®ã¿ã®å ´åˆã¯ç©ºæ¬„ã«ã™ã‚‹
            if company_name.strip() in ['æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'Studio', 'Inc', 'Corp']:
                company_name = ''
            info['company'] = company_name.strip()
            break
    # æ—¥ä»˜æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯å¼·åŒ–
    date_patterns = [
        r'(20[0-9]{2})[å¹´/\-\.](1[0-2]|0?[1-9])[æœˆ/\-\.](3[01]|[12][0-9]|0?[1-9])[æ—¥]?',  # 2019å¹´10æœˆ11æ—¥
        r'(20[0-9]{2})[/-](1[0-2]|0?[1-9])[/-](3[01]|[12][0-9]|0?[1-9])',  # 2019/10/11
        r'(1[0-2]|0?[1-9])[æœˆ/\-\.](3[01]|[12][0-9]|0?[1-9])[æ—¥]?',  # 10æœˆ11æ—¥
    ]
    for pattern in date_patterns:
        for line in lines:
            # é›»è©±ç•ªå·ã‚„No.ãªã©ã‚’é™¤å¤–
            if re.search(r'(é›»è©±|TEL|No\.|NO\.|ãƒ¬ã‚¸|ä¼šè¨ˆ|åº—|\d{4,}-\d{2,}-\d{2,}|\d{2,}-\d{4,}-\d{4,})', line, re.IGNORECASE):
                continue
            match = re.search(pattern, line)
            if match:
                if len(match.groups()) == 3:
                    year, month, day = match.groups()
                    if len(year) == 4:
                        info['date'] = f"{year}/{month.zfill(2)}/{day.zfill(2)}"
                    else:
                        current_year = datetime.now().year
                        info['date'] = f"{current_year}/{year.zfill(2)}/{month.zfill(2)}"
                break
        if info['date']:
            break
    # æœŸé–“æƒ…å ±ï¼ˆxæœˆåˆ†ã€ä¸ŠæœŸåˆ†ã€ä¸‹æœŸåˆ†ãªã©ï¼‰ã‚’æŠ½å‡º
    period_hint = None
    period_match = re.search(r'([0-9]{1,2}æœˆåˆ†|ä¸ŠæœŸåˆ†|ä¸‹æœŸåˆ†|\d{1,2}æœˆåˆ†)', text)
    if period_match:
        period_hint = period_match.group(1)
    
    # é‡‘é¡æŠ½å‡ºï¼šãƒ©ãƒ™ãƒ«å„ªå…ˆãƒ»é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒ»æœ€ä¸‹éƒ¨å„ªå…ˆãƒ»ç¯„å›²ãƒ»AIã‚¯ãƒ­ã‚¹ãƒã‚§ãƒƒã‚¯
    amount_ai = guess_amount_ai(text)
    label_keywords = r'(åˆè¨ˆ|å°è¨ˆ|ç·é¡|ã”è«‹æ±‚é‡‘é¡|è«‹æ±‚é‡‘é¡|åˆè¨ˆé‡‘é¡)'
    exclude_keywords = r'(ãŠé ã‚Š|ãŠé ã‹ã‚Š|ãŠé‡£ã‚Š|ç¾é‡‘|é‡£éŠ­|ã¤ã‚ŠéŠ­)'
    # ç¨ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€è¡Œã‚‚é™¤å¤–
    tax_label_keywords = r'(å†…æ¶ˆè²»ç¨|æ¶ˆè²»ç¨ç­‰|æ¶ˆè²»ç¨|ç¨ç‡|å†…ç¨|å¤–ç¨|ç¨é¡)'
    label_amounts = []
    for i, line in enumerate(lines):
        if re.search(label_keywords, line) and not re.search(exclude_keywords, line) and not re.search(tax_label_keywords, line):
            amount_patterns = [r'([0-9,]+)å††', r'Â¥([0-9,]+)', r'([0-9,]+)']
            for pattern in amount_patterns:
                matches = re.findall(pattern, line)
                for match in matches:
                    if isinstance(match, tuple):
                        match = [x for x in match if x][0] if any(match) else None
                    if match and match.replace(',', '').isdigit():
                        val = int(match.replace(',', ''))
                        if len(str(val)) >= 10:
                            continue
                        if is_year_number(val, line):
                            continue
                        if 1 <= val <= 10000000:
                            label_amounts.append((i, val))
    label_amount = label_amounts[-1][1] if label_amounts else None
    amount_candidates = []
    for i, line in enumerate(lines):
        if re.search(exclude_keywords, line) or re.search(tax_label_keywords, line):
            continue
        for pattern in [r'([0-9,]+)å††', r'Â¥([0-9,]+)']:
            matches = re.findall(pattern, line)
            for m in matches:
                if isinstance(m, tuple):
                    m = [x for x in m if x][0] if any(m) else None
                if m and m.replace(',', '').isdigit():
                    val = int(m.replace(',', ''))
                    if len(str(val)) >= 10:
                        continue
                    if is_year_number(val, line):
                        continue
                    if 1 <= val <= 10000000:
                        amount_candidates.append(val)
    # ãƒ¬ã‚·ãƒ¼ãƒˆä¸‹éƒ¨ã®ç¨é¡è¨˜è¼‰ã‚’å„ªå…ˆ
    bottom_tax_8 = re.search(r'å†…[\sã€€]*8[%ï¼…][^\d]*(?:\\?[0-9,]+)[^\d]*(?:ç¨é¡[\sã€€]*\\?([0-9,]+))', text)
    bottom_tax_10 = re.search(r'å†…[\sã€€]*10[%ï¼…][^\d]*(?:\\?[0-9,]+)[^\d]*(?:ç¨é¡[\sã€€]*\\?([0-9,]+))', text)
    tax_8 = int(bottom_tax_8.group(1).replace(',', '')) if bottom_tax_8 and bottom_tax_8.group(1) else None
    tax_10 = int(bottom_tax_10.group(1).replace(',', '')) if bottom_tax_10 and bottom_tax_10.group(1) else None
    # AIå€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    def is_in_exclude_line(val):
        for line in lines:
            if str(val) in line and re.search(exclude_keywords, line):
                return True
        return False
    if amount_ai:
        if is_year_number(amount_ai, text):
            amount_ai = None
        elif is_in_exclude_line(amount_ai):
            amount_ai = None
        elif not (1 <= amount_ai <= 10000000):
            amount_ai = None
    # --- ç¨åŒºåˆ†ãƒ»ç¨é¡åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’å•†ç¿’æ…£ã«åˆã‚ã›ã¦å¼·åŒ– ---
    text_lower = text.lower()
    # æ˜è¨˜ãŒã‚ã‚Œã°å„ªå…ˆ
    if re.search(r'å¤–ç¨|åˆ¥é€”æ¶ˆè²»ç¨|tax out|tax-out|taxout|ç¨æŠœ|æœ¬ä½“ä¾¡æ ¼', text_lower):
        default_tax_mode = 'å¤–ç¨'
    elif re.search(r'å†…ç¨|ç¨è¾¼|æ¶ˆè²»ç¨è¾¼|tax in|tax-in|taxin', text_lower):
        default_tax_mode = 'å†…ç¨'
    # ã€Œæ¶ˆè²»ç¨ã€ã‚„ã€Œç¨é¡ã€æ¬„ãŒã‚ã‚Šã€ã‹ã¤0å††ã‚„ç©ºæ¬„ãªã‚‰å†…ç¨
    elif re.search(r'æ¶ˆè²»ç¨|ç¨é¡', text) and re.search(r'0å††|Â¥0|0$', text):
        default_tax_mode = 'å†…ç¨'
    else:
        # æ˜è¨˜ãŒãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å†…ç¨
        default_tax_mode = 'å†…ç¨'

    # é‡‘é¡æ±ºå®šå¾Œã®ç¨é¡è¨ˆç®—ã«åæ˜ 
    # æœ€çµ‚çš„ãªé‡‘é¡æ±ºå®š
    amount = None
    if amount_ai and not is_in_exclude_line(amount_ai):
        if label_amount and amount_ai == label_amount:
            amount = amount_ai
        elif not label_amount:
            amount = amount_ai
    if not amount and label_amount:
        amount = label_amount
    if not amount and amount_candidates:
        amount = max(amount_candidates)
    if amount:
        info['amount'] = str(amount)
        # ç¨åŒºåˆ†åˆ¤å®š
        if tax_mode == 'å†…ç¨10%':
            info['tax'] = str(tax_10 if tax_10 is not None else (amount - int(round(amount / 1.1))))
        elif tax_mode == 'å¤–ç¨10%':
            info['tax'] = str(tax_10 if tax_10 is not None else int(amount * 0.1))
        elif tax_mode == 'å†…ç¨8%':
            info['tax'] = str(tax_8 if tax_8 is not None else (amount - int(round(amount / 1.08))))
        elif tax_mode == 'å¤–ç¨8%':
            info['tax'] = str(tax_8 if tax_8 is not None else int(amount * 0.08))
        elif tax_mode == 'éèª²ç¨':
            info['tax'] = '0'
        else:
            # æ˜è¨˜ãŒãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å†…ç¨
            if default_tax_mode == 'å†…ç¨':
                if '8%' in text or 'ï¼˜ï¼…' in text:
                    info['tax'] = str(tax_8 if tax_8 is not None else (amount - int(round(amount / 1.08))))
                else:
                    info['tax'] = str(tax_10 if tax_10 is not None else (amount - int(round(amount / 1.1))))
            else:
                if '8%' in text or 'ï¼˜ï¼…' in text:
                    info['tax'] = str(tax_8 if tax_8 is not None else int(amount * 0.08))
                else:
                    info['tax'] = str(tax_10 if tax_10 is not None else int(amount * 0.1))
    
    # æ‘˜è¦ã‚’AIã§ç”Ÿæˆï¼ˆæœŸé–“æƒ…å ±ã¨è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã™ï¼‰
    info['description'] = guess_description_ai(text, period_hint, extra_prompt=extra_prompt)
    
    # ã¾ãšAIã§æ¨æ¸¬
    account_ai = guess_account_ai(text, stance, extra_prompt=extra_prompt)
    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§æ¨æ¸¬
    if account_ai:
        info['account'] = account_ai
        info['account_source'] = 'AI'
    else:
        # é£²æ–™ãƒ»é£Ÿå“ç³»ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ä¼šè­°è²»ã¾ãŸã¯æ¶ˆè€—å“è²»
        if re.search(r'é£²æ–™|é£Ÿå“|ãŠè“å­|ãƒšãƒƒãƒˆãƒœãƒˆãƒ«|å¼å½“|ãƒ‘ãƒ³|ã‚³ãƒ¼ãƒ’ãƒ¼|ãŠèŒ¶|æ°´|ã‚¸ãƒ¥ãƒ¼ã‚¹', text):
            info['account'] = 'ä¼šè­°è²»'
        elif stance == 'issued':
            if 'å£²ä¸Š' in text or 'è«‹æ±‚' in text or 'ç´å“' in text:
                info['account'] = 'å£²ä¸Šé«˜'
            else:
                info['account'] = 'é›‘åå…¥'
        else:
            if 'è¬›ç¾©' in text or 'ç ”ä¿®' in text:
                info['account'] = 'ç ”ä¿®è²»'
            elif 'äº¤é€š' in text or 'ã‚¿ã‚¯ã‚·ãƒ¼' in text:
                info['account'] = 'æ—…è²»äº¤é€šè²»'
            elif 'é€šä¿¡' in text or 'é›»è©±' in text:
                info['account'] = 'é€šä¿¡è²»'
            elif 'äº‹å‹™ç”¨å“' in text or 'æ–‡å…·' in text:
                info['account'] = 'æ¶ˆè€—å“è²»'
            else:
                info['account'] = 'ä»®æ‰•é‡‘'
        info['account_source'] = 'ãƒ«ãƒ¼ãƒ«'
    return info

# ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ç”¨ã‚«ãƒ©ãƒ 
MF_COLUMNS = [
    'å–å¼•No', 'å–å¼•æ—¥', 'å€Ÿæ–¹å‹˜å®šç§‘ç›®', 'å€Ÿæ–¹è£œåŠ©ç§‘ç›®', 'å€Ÿæ–¹éƒ¨é–€', 'å€Ÿæ–¹å–å¼•å…ˆ', 'å€Ÿæ–¹ç¨åŒºåˆ†', 'å€Ÿæ–¹ã‚¤ãƒ³ãƒœã‚¤ã‚¹', 'å€Ÿæ–¹é‡‘é¡(å††)', 'å€Ÿæ–¹ç¨é¡',
    'è²¸æ–¹å‹˜å®šç§‘ç›®', 'è²¸æ–¹è£œåŠ©ç§‘ç›®', 'è²¸æ–¹éƒ¨é–€', 'è²¸æ–¹å–å¼•å…ˆ', 'è²¸æ–¹ç¨åŒºåˆ†', 'è²¸æ–¹ã‚¤ãƒ³ãƒœã‚¤ã‚¹', 'è²¸æ–¹é‡‘é¡(å††)', 'è²¸æ–¹ç¨é¡',
    'æ‘˜è¦', 'ä»•è¨³ãƒ¡ãƒ¢', 'ã‚¿ã‚°', 'MFä»•è¨³ã‚¿ã‚¤ãƒ—', 'æ±ºç®—æ•´ç†ä»•è¨³', 'ä½œæˆæ—¥æ™‚', 'ä½œæˆè€…', 'æœ€çµ‚æ›´æ–°æ—¥æ™‚', 'æœ€çµ‚æ›´æ–°è€…'
]

# ç¨åŒºåˆ†è‡ªå‹•åˆ¤å®šé–¢æ•°ã‚’è¿½åŠ 
def guess_tax_category(text, info, is_debit=True):
    # 10%ã‚„æ¶ˆè²»ç¨ã®ãƒ¯ãƒ¼ãƒ‰ã§åˆ¤å®š
    if 'å£²ä¸Š' in info.get('account', ''):
        if '10%' in text or 'æ¶ˆè²»ç¨' in text:
            return 'èª²ç¨å£²ä¸Š 10%'
        elif '8%' in text:
            return 'èª²ç¨å£²ä¸Š 8%'
        elif 'éèª²ç¨' in text:
            return 'éèª²ç¨'
        elif 'å…ç¨' in text:
            return 'å…ç¨'
        else:
            return 'å¯¾è±¡å¤–'
    else:
        if '10%' in text or 'æ¶ˆè²»ç¨' in text:
            return 'èª²ç¨ä»•å…¥ 10%'
        elif '8%' in text:
            return 'èª²ç¨ä»•å…¥ 8%'
        elif 'éèª²ç¨' in text:
            return 'éèª²ç¨'
        elif 'å…ç¨' in text:
            return 'å…ç¨'
        else:
            return 'å¯¾è±¡å¤–'

# åå…¥/æ”¯å‡ºåˆ¤å®šã¨MFç”¨ä»•è¨³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

def create_mf_journal_row(info):
    try:
        amount = int(info['amount']) if info['amount'] else 0
    except Exception:
        amount = 0
    if info['account'] in ['ç ”ä¿®è²»', 'æ•™è‚²ç ”ä¿®è²»', 'æ—…è²»äº¤é€šè²»', 'é€šä¿¡è²»', 'æ¶ˆè€—å“è²»', 'ä¼šè­°è²»', 'äº¤éš›è²»', 'åºƒå‘Šå®£ä¼è²»', 'å¤–æ³¨è²»', 'æ”¯æ‰•æ‰‹æ•°æ–™', 'ä»®æ‰•é‡‘', 'ä¿®ç¹•è²»', 'ä»•å…¥é«˜', 'æ¸›ä¾¡å„Ÿå´è²»']:
        debit_account = info['account']
        credit_account = 'ç¾é‡‘'
        debit_amount = amount
        credit_amount = amount
    elif info['account'] in ['å£²ä¸Šé«˜', 'é›‘åå…¥', 'å—å–æ‰‹å½¢', 'å£²æ›é‡‘']:
        debit_account = 'ç¾é‡‘'
        credit_account = info['account']
        debit_amount = amount
        credit_amount = amount
    else:
        debit_account = info['account']
        credit_account = 'ç¾é‡‘'
        debit_amount = amount
        credit_amount = amount
    tag = 'AIæ¨æ¸¬' if info.get('account_source') == 'AI' else 'ãƒ«ãƒ¼ãƒ«æ¨æ¸¬'
    # ç¨åŒºåˆ†è‡ªå‹•åˆ¤å®šï¼ˆOCRå…¨æ–‡ã‚’ä½¿ã†ï¼‰
    ocr_text = info.get('ocr_text', '')
    debit_tax = guess_tax_category(ocr_text, info, is_debit=True)
    credit_tax = guess_tax_category(ocr_text, info, is_debit=False)
    row = [
        '',
        info['date'],
        debit_account, '', '', '', debit_tax, '', debit_amount, info['tax'],
        credit_account, '', '', '', credit_tax, '', credit_amount, '0',
        info['description'], '', tag, '', '', '', '', '', '', ''
    ]
    if len(row) < len(MF_COLUMNS):
        row += [''] * (len(MF_COLUMNS) - len(row))
    elif len(row) > len(MF_COLUMNS):
        row = row[:len(MF_COLUMNS)]
    return row

# æ—¢å­˜ã®generate_csvã‚’æ‹¡å¼µ
def generate_csv(info_list, output_filename, mode='default', as_txt=False):
    if mode == 'mf':
        rows = [MF_COLUMNS]
        for info in info_list:
            rows.append(create_mf_journal_row(info))
        df = pd.DataFrame(data=rows[1:], columns=rows[0])
        file_extension = '.txt' if as_txt else '.csv'
        output_path = os.path.join('output', output_filename + file_extension)
        if as_txt:
            df.to_csv(output_path, index=False, header=True, encoding='utf-8-sig')
        else:
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        # è¾æ›¸å½¢å¼ã§æƒ…å ±ã‚’è¿”ã™
        return {
            'path': output_path,
            'filename': output_filename + file_extension,
            'mime_type': 'text/plain' if as_txt else 'text/csv'
        }
    else:
        df = pd.DataFrame(info_list)
        df = df[['date', 'account', 'account_source', 'amount', 'tax', 'company', 'description']]
        df.columns = ['å–å¼•æ—¥', 'å‹˜å®šç§‘ç›®', 'æ¨æ¸¬æ–¹æ³•', 'é‡‘é¡', 'æ¶ˆè²»ç¨', 'å–å¼•å…ˆ', 'æ‘˜è¦']
        file_extension = '.txt' if as_txt else '.csv'
        output_path = os.path.join('output', output_filename + file_extension)
        if as_txt:
            df.to_csv(output_path, index=False, header=True, encoding='utf-8-sig')
        else:
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        # è¾æ›¸å½¢å¼ã§æƒ…å ±ã‚’è¿”ã™
        return {
            'path': output_path,
            'filename': output_filename + file_extension,
            'mime_type': 'text/plain' if as_txt else 'text/csv'
        }

# ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ã®é–¢æ•°
def save_review_to_firestore(original_text, ai_journal, corrected_journal, reviewer_name, comments=""):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼å†…å®¹ã‚’Firestoreã«ä¿å­˜"""
    if db is None:
        st.error("Firebaseæ¥ç¶šãŒç¢ºç«‹ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
    
    try:
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not original_text or not ai_journal or not corrected_journal:
            st.error("å¿…é ˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            return False
        
        if not reviewer_name or reviewer_name.strip() == "":
            reviewer_name = "åŒ¿å"
        
        review_data = {
            'original_text': original_text,
            'ai_journal': ai_journal,
            'corrected_journal': corrected_journal,
            'reviewer_name': reviewer_name.strip(),
            'comments': comments.strip() if comments else "",
            'timestamp': datetime.now(),
            'is_corrected': ai_journal != corrected_journal
        }
        
        # reviewsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ä¿å­˜
        doc_ref = db.collection('reviews').add(review_data)
        st.success(f"âœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ID: {doc_ref[1].id}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ï¼ˆæ–°ã—ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¿½åŠ ã•ã‚ŒãŸãŸã‚ï¼‰
        cache_key = 'learning_data_cache'
        cache_timestamp_key = 'learning_data_timestamp'
        if cache_key in st.session_state:
            del st.session_state[cache_key]
        if cache_timestamp_key in st.session_state:
            del st.session_state[cache_timestamp_key]
        
        return True
    except Exception as e:
        st.error(f"âŒ ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.error("è©³ç´°: Firebaseæ¥ç¶šã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return False

def get_similar_reviews(text, limit=5):
    """é¡ä¼¼ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ï¼ˆRAGç”¨ï¼‰"""
    if db is None:
        return []
    
    try:
        # å…¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ï¼ˆå°†æ¥çš„ã«ã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«å¤‰æ›´ï¼‰
        reviews_ref = db.collection('reviews').limit(limit).stream()
        reviews = []
        for doc in reviews_ref:
            review_data = doc.to_dict()
            # ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå°†æ¥çš„ã«ã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«å¤‰æ›´ï¼‰
            if any(keyword in text.lower() for keyword in review_data.get('original_text', '').lower().split()):
                reviews.append(review_data)
        return reviews
    except Exception as e:
        st.warning(f"é¡ä¼¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def get_all_reviews_for_learning():
    """å­¦ç¿’ç”¨ã«å…¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if db is None:
        return []
    
    try:
        reviews_ref = db.collection('reviews').stream()
        reviews = []
        for doc in reviews_ref:
            review_data = doc.to_dict()
            review_data['doc_id'] = doc.id
            reviews.append(review_data)
        return reviews
    except Exception as e:
        st.warning(f"å…¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def extract_correction_patterns(reviews):
    """ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ±è¨ˆçš„ã«æŠ½å‡º"""
    if not reviews:
        return {}
    
    patterns = {}
    
    for review in reviews:
        if not review.get('is_corrected', False):
            continue
            
        ai_journal = review.get('ai_journal', '')
        corrected_journal = review.get('corrected_journal', '')
        
        # AIæ¨æ¸¬ã¨ä¿®æ­£å¾Œã®å‹˜å®šç§‘ç›®ã‚’æŠ½å‡º
        ai_account = extract_account_from_journal(ai_journal)
        corrected_account = extract_account_from_journal(corrected_journal)
        
        if ai_account and corrected_account and ai_account != corrected_account:
            pattern_key = f"{ai_account} â†’ {corrected_account}"
            if pattern_key not in patterns:
                patterns[pattern_key] = {
                    'count': 0,
                    'examples': [],
                    'keywords': set()
                }
            
            patterns[pattern_key]['count'] += 1
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            original_text = review.get('original_text', '').lower()
            keywords = extract_keywords_from_text(original_text)
            patterns[pattern_key]['keywords'].update(keywords)
            
            # ä¾‹ã‚’ä¿å­˜ï¼ˆæœ€å¤§5ä¾‹ã¾ã§ï¼‰
            if len(patterns[pattern_key]['examples']) < 5:
                patterns[pattern_key]['examples'].append({
                    'text': original_text[:100] + "..." if len(original_text) > 100 else original_text,
                    'comments': review.get('comments', '')
                })
    
    return patterns

def extract_account_from_journal(journal_text):
    """ä»•è¨³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹˜å®šç§‘ç›®ã‚’æŠ½å‡º"""
    if 'å‹˜å®šç§‘ç›®:' in journal_text:
        account_part = journal_text.split('å‹˜å®šç§‘ç›®:')[1].split(',')[0].strip()
        return account_part
    return None

def extract_keywords_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå°†æ¥çš„ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªNLPã‚’ä½¿ç”¨ï¼‰
    keywords = set()
    
    # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
    import re
    amounts = re.findall(r'\d{1,3}(?:,\d{3})*å††', text)
    keywords.update(amounts)
    
    # ä¼šç¤¾åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã®å€™è£œ
    words = text.split()
    for word in words:
        if len(word) > 2 and any(char in word for char in ['æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è²»', 'æ–™']):
            keywords.add(word)
    
    return keywords

def generate_advanced_learning_prompt(text, reviews):
    """é«˜åº¦ãªå­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    if not reviews:
        return ""
    
    # ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
    patterns = extract_correction_patterns(reviews)
    
    # çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ
    total_reviews = len(reviews)
    corrected_reviews = sum(1 for r in reviews if r.get('is_corrected', False))
    accuracy_rate = ((total_reviews - corrected_reviews) / total_reviews * 100) if total_reviews > 0 else 0
    
    # é »å‡ºã™ã‚‹ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
    frequent_patterns = {k: v for k, v in patterns.items() if v['count'] >= 2}
    
    learning_prompt = f"\n\nã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘\n"
    learning_prompt += f"ç·ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {total_reviews}ä»¶\n"
    learning_prompt += f"ä¿®æ­£ã•ã‚ŒãŸä»•è¨³: {corrected_reviews}ä»¶\n"
    learning_prompt += f"ç¾åœ¨ã®æ­£è§£ç‡: {accuracy_rate:.1f}%\n"
    
    if frequent_patterns:
        learning_prompt += f"\nã€é »å‡ºä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘\n"
        for pattern, data in sorted(frequent_patterns.items(), key=lambda x: x[1]['count'], reverse=True)[:5]:
            learning_prompt += f"â€¢ {pattern} ({data['count']}å›)\n"
            if data['examples']:
                example = data['examples'][0]
                learning_prompt += f"  ä¾‹: {example['text']}\n"
                if example['comments']:
                    learning_prompt += f"  ç†ç”±: {example['comments']}\n"
    
    # é¡ä¼¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ¤œç´¢
    similar_reviews = find_similar_reviews_advanced(text, reviews)
    
    if similar_reviews:
        learning_prompt += f"\nã€é¡ä¼¼ä¿®æ­£ä¾‹ã€‘\n"
        for i, review in enumerate(similar_reviews[:3], 1):
            ai_journal = review.get('ai_journal', '')
            corrected_journal = review.get('corrected_journal', '')
            comments = review.get('comments', '')
            
            learning_prompt += f"ä¾‹{i}:\n"
            learning_prompt += f"AIæ¨æ¸¬: {ai_journal}\n"
            learning_prompt += f"æ­£è§£: {corrected_journal}\n"
            if comments:
                learning_prompt += f"ä¿®æ­£ç†ç”±: {comments}\n"
            learning_prompt += "\n"
    
    learning_prompt += "ä¸Šè¨˜ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å‚è€ƒã«ã€ã‚ˆã‚Šæ­£ç¢ºãªä»•è¨³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
    
    return learning_prompt

def find_similar_reviews_advanced(text, reviews):
    """é«˜åº¦ãªé¡ä¼¼ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¤œç´¢"""
    if not reviews:
        return []
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã‚’æŠ½å‡º
    text_features = extract_text_features(text)
    
    similarities = []
    for review in reviews:
        if not review.get('is_corrected', False):
            continue
            
        review_features = extract_text_features(review.get('original_text', ''))
        similarity_score = calculate_similarity(text_features, review_features)
        
        if similarity_score > 0.3:  # é¡ä¼¼åº¦é–¾å€¤
            similarities.append((similarity_score, review))
    
    # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
    similarities.sort(key=lambda x: x[0], reverse=True)
    
    return [review for score, review in similarities[:5]]

def extract_text_features(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã‚’æŠ½å‡º"""
    features = {
        'keywords': set(),
        'amounts': [],
        'companies': set(),
        'services': set()
    }
    
    import re
    
    # é‡‘é¡ã‚’æŠ½å‡º
    amounts = re.findall(r'\d{1,3}(?:,\d{3})*å††', text)
    features['amounts'] = amounts
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    words = text.lower().split()
    for word in words:
        if len(word) > 2:
            features['keywords'].add(word)
    
    # ä¼šç¤¾åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æŠ½å‡º
    company_patterns = ['æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾', 'ã‚µãƒ¼ãƒ“ã‚¹', 'äº‹å‹™æ‰€', 'ã‚»ãƒ³ã‚¿ãƒ¼']
    for pattern in company_patterns:
        if pattern in text:
            features['companies'].add(pattern)
    
    return features

def calculate_similarity(features1, features2):
    """2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¤‡åº¦
    keyword_overlap = len(features1['keywords'] & features2['keywords'])
    keyword_union = len(features1['keywords'] | features2['keywords'])
    keyword_similarity = keyword_overlap / keyword_union if keyword_union > 0 else 0
    
    # é‡‘é¡ã®é¡ä¼¼åº¦
    amount_similarity = 0
    if features1['amounts'] and features2['amounts']:
        # é‡‘é¡ç¯„å›²ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        amounts1 = [int(amt.replace(',', '').replace('å††', '')) for amt in features1['amounts']]
        amounts2 = [int(amt.replace(',', '').replace('å††', '')) for amt in features2['amounts']]
        
        if amounts1 and amounts2:
            avg1 = sum(amounts1) / len(amounts1)
            avg2 = sum(amounts2) / len(amounts2)
            amount_diff = abs(avg1 - avg2) / max(avg1, avg2) if max(avg1, avg2) > 0 else 1
            amount_similarity = 1 - min(amount_diff, 1)
    
    # ä¼šç¤¾ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®é‡è¤‡åº¦
    company_overlap = len(features1['companies'] & features2['companies'])
    company_union = len(features1['companies'] | features2['companies'])
    company_similarity = company_overlap / company_union if company_union > 0 else 0
    
    # ç·åˆé¡ä¼¼åº¦
    total_similarity = (keyword_similarity * 0.5 + amount_similarity * 0.3 + company_similarity * 0.2)
    
    return total_similarity

def generate_learning_prompt_from_reviews(text, similar_reviews):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    if not similar_reviews:
        return ""
    
    learning_examples = []
    for review in similar_reviews:
        original_text = review.get('original_text', '')
        ai_journal = review.get('ai_journal', '')
        corrected_journal = review.get('corrected_journal', '')
        comments = review.get('comments', '')
        
        # ä¿®æ­£ãŒã‚ã£ãŸå ´åˆã®ã¿å­¦ç¿’ä¾‹ã¨ã—ã¦è¿½åŠ 
        if review.get('is_corrected', False) and ai_journal != corrected_journal:
            learning_examples.append({
                'original_text': original_text[:200] + "..." if len(original_text) > 200 else original_text,
                'ai_journal': ai_journal,
                'corrected_journal': corrected_journal,
                'comments': comments
            })
    
    if not learning_examples:
        return ""
    
    # å­¦ç¿’ä¾‹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›
    learning_prompt = "\n\nã€éå»ã®ä¿®æ­£ä¾‹ã‹ã‚‰å­¦ç¿’ã€‘\n"
    learning_prompt += "ä»¥ä¸‹ã®ä¿®æ­£ä¾‹ã‚’å‚è€ƒã«ã—ã¦ã€ã‚ˆã‚Šæ­£ç¢ºãªä»•è¨³ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š\n"
    
    for i, example in enumerate(learning_examples[:3], 1):  # æœ€å¤§3ä¾‹ã¾ã§
        learning_prompt += f"\nä¾‹{i}:\n"
        learning_prompt += f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {example['original_text']}\n"
        learning_prompt += f"AIæ¨æ¸¬: {example['ai_journal']}\n"
        learning_prompt += f"æ­£è§£: {example['corrected_journal']}\n"
        if example['comments']:
            learning_prompt += f"ä¿®æ­£ç†ç”±: {example['comments']}\n"
    
    learning_prompt += "\nä¸Šè¨˜ã®ä¿®æ­£ä¾‹ã‚’å‚è€ƒã«ã€ä»Šå›ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ã‚ˆã‚Šæ­£ç¢ºãªä»•è¨³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
    
    return learning_prompt

def get_cached_learning_data():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    cache_key = 'learning_data_cache'
    cache_timestamp_key = 'learning_data_timestamp'
    
    if cache_key in st.session_state and cache_timestamp_key in st.session_state:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ1æ™‚é–“ï¼‰
        cache_age = time.time() - st.session_state[cache_timestamp_key]
        if cache_age < 3600:  # 1æ™‚é–“ = 3600ç§’
            return st.session_state[cache_key]
    
    return None

def set_cached_learning_data(learning_data):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache_key = 'learning_data_cache'
    cache_timestamp_key = 'learning_data_timestamp'
    
    st.session_state[cache_key] = learning_data
    st.session_state[cache_timestamp_key] = time.time()

def prepare_learning_data_for_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
    try:
        reviews = get_all_reviews_for_learning()
        if not reviews:
            return None
        
        return {
            'reviews': reviews,
            'total_reviews': len(reviews),
            'timestamp': time.time()
        }
    except Exception as e:
        st.warning(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def generate_cached_learning_prompt(text, cached_data):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    if not cached_data or not cached_data.get('reviews'):
        return ""
    
    try:
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨
        vector_model = None
        if VECTOR_SEARCH_AVAILABLE:
            vector_model = initialize_vector_model()
        
        similar_reviews = hybrid_search_similar_reviews(
            text, 
            cached_data['reviews'], 
            vector_model, 
            top_k=5
        )
        
        return generate_hybrid_learning_prompt(text, similar_reviews)
    except Exception as e:
        st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return ""

def guess_account_ai_with_learning(text, stance='received', extra_prompt=''):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸAIæ¨æ¸¬ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
    if not OPENAI_API_KEY:
        st.warning("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIæ¨æ¸¬ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        return None
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    cached_learning_data = get_cached_learning_data()
    
    if cached_learning_data:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
        learning_prompt = generate_cached_learning_prompt(text, cached_learning_data)
        cache_status = f"ğŸ“š ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ ({cached_learning_data['total_reviews']}ä»¶)"
    else:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ãªå ´åˆã¯æ–°ã—ãå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        learning_data = prepare_learning_data_for_cache()
        if learning_data:
            set_cached_learning_data(learning_data)
            learning_prompt = generate_cached_learning_prompt(text, learning_data)
            cache_status = f"ğŸ”„ æ–°ã—ã„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã—ãŸ ({learning_data['total_reviews']}ä»¶)"
        else:
            learning_prompt = ""
            cache_status = "âš ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ"
    
    if stance == 'issued':
        stance_prompt = "ã‚ãªãŸã¯è«‹æ±‚æ›¸ã‚’ç™ºè¡Œã—ãŸå´ï¼ˆå£²ä¸Šè¨ˆä¸Šå´ï¼‰ã®çµŒç†æ‹…å½“è€…ã§ã™ã€‚å£²ä¸Šãƒ»åå…¥ã«è©²å½“ã™ã‚‹å‹˜å®šç§‘ç›®ã®ã¿ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
        account_list = "å£²ä¸Šé«˜ã€é›‘åå…¥ã€å—å–æ‰‹å½¢ã€å£²æ›é‡‘"
    else:
        stance_prompt = "ã‚ãªãŸã¯è«‹æ±‚æ›¸ã‚’å—é ˜ã—ãŸå´ï¼ˆè²»ç”¨è¨ˆä¸Šå´ï¼‰ã®çµŒç†æ‹…å½“è€…ã§ã™ã€‚è²»ç”¨ãƒ»ä»•å…¥ãƒ»è²©ç®¡è²»ã«è©²å½“ã™ã‚‹å‹˜å®šç§‘ç›®ã®ã¿ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
        account_list = "ç ”ä¿®è²»ã€æ•™è‚²ç ”ä¿®è²»ã€æ—…è²»äº¤é€šè²»ã€é€šä¿¡è²»ã€æ¶ˆè€—å“è²»ã€ä¼šè­°è²»ã€äº¤éš›è²»ã€åºƒå‘Šå®£ä¼è²»ã€å¤–æ³¨è²»ã€æ”¯æ‰•æ‰‹æ•°æ–™ã€ä»®æ‰•é‡‘ã€ä¿®ç¹•è²»ã€ä»•å…¥é«˜ã€æ¸›ä¾¡å„Ÿå´è²»"
    
    prompt = (
        f"{stance_prompt}\n"
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯é ˜åæ›¸ã‚„è«‹æ±‚æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚\n"
        f"å¿…ãšä¸‹è¨˜ã®å‹˜å®šç§‘ç›®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã‚’1ã¤ã ã‘æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "\nã€å‹˜å®šç§‘ç›®ãƒªã‚¹ãƒˆã€‘\n{account_list}\n"
        "\næ‘˜è¦ã‚„å•†å“åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åãƒ»è¬›ç¾©åã‚’ãã®ã¾ã¾å‹˜å®šç§‘ç›®ã«ã—ãªã„ã§ãã ã•ã„ã€‚\n"
        "ãŸã¨ãˆã°ã€SNSè¬›ç¾©è²»ã€ã‚„ã€â—‹â—‹ã‚»ãƒŸãƒŠãƒ¼è²»ã€ãªã©ã¯ã€ç ”ä¿®è²»ã€ã‚„ã€æ•™è‚²ç ”ä¿®è²»ã€ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n"
        "åˆ†ã‹ã‚‰ãªã„å ´åˆã¯å¿…ãšã€ä»®æ‰•é‡‘ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "\nâ€»ã€ãƒ¬ã‚¿ãƒ¼ãƒ‘ãƒƒã‚¯ã€ã€åˆ‡æ‰‹ã€ã€éƒµä¾¿ã€ã€ã‚†ã†ãƒ‘ãƒƒã‚¯ã€ã€ã‚†ã†ãƒ¡ãƒ¼ãƒ«ã€ã€ã‚†ã†ãƒ‘ã‚±ãƒƒãƒˆã€ã€ã‚¹ãƒãƒ¼ãƒˆãƒ¬ã‚¿ãƒ¼ã€ã€ãƒŸãƒ‹ãƒ¬ã‚¿ãƒ¼ã€ãªã©éƒµä¾¿ãƒ»é…é€ã‚µãƒ¼ãƒ“ã‚¹ã«è©²å½“ã™ã‚‹å ´åˆã¯å¿…ãšã€é€šä¿¡è²»ã€ã¨ã—ã¦ãã ã•ã„ã€‚\n"
        "â€»ã€é£²æ–™ã€ã€é£Ÿå“ã€ã€ãŠè“å­ã€ã€ãƒšãƒƒãƒˆãƒœãƒˆãƒ«ã€ã€å¼å½“ã€ã€ãƒ‘ãƒ³ã€ã€ã‚³ãƒ¼ãƒ’ãƒ¼ã€ã€ãŠèŒ¶ã€ã€æ°´ã€ã€ã‚¸ãƒ¥ãƒ¼ã‚¹ã€ãªã©é£²é£Ÿç‰©ã‚„è»½é£Ÿãƒ»ä¼šè­°ç”¨ã®é£Ÿã¹ç‰©ãƒ»é£²ã¿ç‰©ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã€ä¼šè­°è²»ã¾ãŸã¯æ¶ˆè€—å“è²»ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚\n"
        "\nã€è‰¯ã„ä¾‹ã€‘\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: SNSè¬›ç¾©è²» 10,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šç ”ä¿®è²»\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒ¬ã‚¿ãƒ¼ãƒ‘ãƒƒã‚¯ãƒ—ãƒ©ã‚¹ 1,200å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šé€šä¿¡è²»\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒšãƒƒãƒˆãƒœãƒˆãƒ«é£²æ–™ãƒ»ãŠè“å­ 2,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šä¼šè­°è²»\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: é£Ÿå“ãƒ»é£²æ–™ãƒ»ãƒ‘ãƒ³ 1,500å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šæ¶ˆè€—å“è²»\n"
        "\nã€æ‚ªã„ä¾‹ã€‘\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: SNSè¬›ç¾©è²» 10,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šSNSè¬›ç¾©è²»ï¼ˆÃ—ï¼‰\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒ¬ã‚¿ãƒ¼ãƒ‘ãƒƒã‚¯ãƒ—ãƒ©ã‚¹ 1,200å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šåºƒå‘Šå®£ä¼è²»ï¼ˆÃ—ï¼‰\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: ãƒšãƒƒãƒˆãƒœãƒˆãƒ«é£²æ–™ãƒ»ãŠè“å­ 2,000å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šé€šä¿¡è²»ï¼ˆÃ—ï¼‰\n"
        "ãƒ†ã‚­ã‚¹ãƒˆ: é£Ÿå“ãƒ»é£²æ–™ãƒ»ãƒ‘ãƒ³ 1,500å††\nâ†’ å‹˜å®šç§‘ç›®ï¼šé€šä¿¡è²»ï¼ˆÃ—ï¼‰\n"
        f"\nã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\nå‹˜å®šç§‘ç›®ï¼š"
    ) + (f"\nã€è¿½åŠ æŒ‡ç¤ºã€‘\n{extra_prompt}" if extra_prompt else "") + learning_prompt
    
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-4.1-nano",
        "messages": [
            {"role": "system", "content": stance_prompt},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 20,
        "temperature": 0
    }
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        account = content.split("\n")[0].replace("å‹˜å®šç§‘ç›®ï¼š", "").strip()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤º
        if learning_prompt:
            st.info(cache_status)
        
        return account
    except Exception as e:
        st.warning(f"AIã«ã‚ˆã‚‹å‹˜å®šç§‘ç›®æ¨æ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_saved_reviews(limit=10):
    """ä¿å­˜ã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if db is None:
        return []
    
    try:
        reviews_ref = db.collection('reviews').limit(limit).stream()
        reviews = []
        for doc in reviews_ref:
            review_data = doc.to_dict()
            review_data['doc_id'] = doc.id
            reviews.append(review_data)
        return reviews
    except Exception as e:
        st.warning(f"ä¿å­˜ã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def export_reviews_to_csv():
    """ä¿å­˜ã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    if db is None:
        return None
    
    try:
        reviews_ref = db.collection('reviews').stream()
        reviews = []
        for doc in reviews_ref:
            review_data = doc.to_dict()
            review_data['doc_id'] = doc.id
            reviews.append(review_data)
        
        if not reviews:
            return None
        
        # DataFrameã«å¤‰æ›
        df_data = []
        for review in reviews:
            df_data.append({
                'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID': review.get('doc_id', ''),
                'ä¿å­˜æ—¥æ™‚': review.get('timestamp', ''),
                'ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‹…å½“è€…': review.get('reviewer_name', ''),
                'ä¿®æ­£ã‚ã‚Š': review.get('is_corrected', False),
                'ã‚³ãƒ¡ãƒ³ãƒˆ': review.get('comments', ''),
                'å…ƒã®AIä»•è¨³': review.get('ai_journal', ''),
                'ä¿®æ­£å¾Œã®ä»•è¨³': review.get('corrected_journal', ''),
                'å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ': review.get('original_text', '')[:500] + '...' if len(review.get('original_text', '')) > 500 else review.get('original_text', '')
            })
        
        df = pd.DataFrame(df_data)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'reviews_export_{timestamp}.csv'
        filepath = os.path.join('output', filename)
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        return {'filename': filename, 'path': filepath, 'mime_type': 'text/csv'}
    except Exception as e:
        st.error(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def extract_text_from_pdf(pdf_bytes):
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        text = ""
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
        return text.strip()
    except Exception:
        return ""

def is_text_sufficient(text):
    # æ—¥æœ¬èªãŒå«ã¾ã‚Œã€é‡‘é¡ã‚„æ—¥ä»˜ãªã©ã®ä¼šè¨ˆæƒ…å ±ãŒã‚ã‚‹ã‹ç°¡æ˜“åˆ¤å®š
    if len(text) < 30:
        return False
    if not re.search(r'[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³]', text):
        return False
    if not re.search(r'\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥|å††|åˆè¨ˆ|é‡‘é¡', text):
        return False
    return True

# PDF.coã§PDFâ†’ç”»åƒåŒ–
import base64

def upload_pdf_to_pdfco(pdf_bytes, api_key):
    url = "https://api.pdf.co/v1/file/upload"
    headers = {"x-api-key": api_key}
    files = {"file": ("file.pdf", pdf_bytes, "application/pdf")}
    response = requests.post(url, headers=headers, files=files)
    result = response.json()
    if not result.get("url"):
        raise Exception(f"PDF.co Upload APIã‚¨ãƒ©ãƒ¼: {result.get('message', 'Unknown error')}")
    return result["url"]

def pdf_to_images_pdfco(pdf_bytes, api_key):
    # 1. ã¾ãšã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    file_url = upload_pdf_to_pdfco(pdf_bytes, api_key)
    # 2. ç”»åƒåŒ–
    url = "https://api.pdf.co/v1/pdf/convert/to/jpg"
    headers = {"x-api-key": api_key}
    params = {"url": file_url}
    response = requests.post(url, headers=headers, json=params)
    result = response.json()
    if result.get("error"):
        raise Exception(f"PDF.co APIã‚¨ãƒ©ãƒ¼: {result.get('message', 'Unknown error')}")
    image_urls = result.get("urls", [])
    if not image_urls:
        raise Exception("PDF.co APIã‚¨ãƒ©ãƒ¼: ç”»åƒURLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    images = []
    for img_url in image_urls:
        img_resp = requests.get(img_url)
        img_resp.raise_for_status()
        images.append(img_resp.content)
    return images

st.title('é ˜åæ›¸ãƒ»è«‹æ±‚æ›¸AIä»•è¨³ Webã‚¢ãƒ—ãƒª')

# Firebaseæ¥ç¶šçŠ¶æ…‹ã®ç°¡æ˜“è¡¨ç¤º
if db is None:
    st.warning("âš ï¸ Firebaseæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
else:
    st.success("âœ… Firebaseæ¥ç¶šãŒç¢ºç«‹ã•ã‚Œã¾ã—ãŸã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã™ã€‚")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– ---
if 'uploaded_files_data' not in st.session_state:
    st.session_state.uploaded_files_data = []
if 'processed_results' not in st.session_state:
    st.session_state.processed_results = []
if 'csv_file_info' not in st.session_state:
    st.session_state.csv_file_info = None
if 'current_stance' not in st.session_state:
    st.session_state.current_stance = 'received'
if 'current_tax_mode' not in st.session_state:
    st.session_state.current_tax_mode = 'è‡ªå‹•åˆ¤å®š'
if 'current_output_mode' not in st.session_state:
    st.session_state.current_output_mode = 'æ±ç”¨CSV'
if 'force_pdf_ocr' not in st.session_state:
    st.session_state.force_pdf_ocr = False

# --- ã‚¿ãƒ–ã«ã‚ˆã‚‹å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é¸æŠ ---
tab1, tab2 = st.tabs(["ğŸ“„ å˜ä¸€å‡¦ç†", "ğŸš€ ãƒãƒƒãƒå‡¦ç†"])

with tab1:
    st.subheader("ğŸ“„ å˜ä¸€å‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
    
    # --- UIã«ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¿½åŠ  ---
    debug_mode = st.sidebar.checkbox('ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰', value=False)

    # ç«‹å ´é¸æŠã‚’è¿½åŠ 
    stance = st.radio('ã“ã®è«‹æ±‚æ›¸ã¯ã©ã¡ã‚‰ã®ç«‹å ´ã§ã™ã‹ï¼Ÿ', ['å—é ˜ï¼ˆè‡ªç¤¾ãŒæ”¯æ‰•ã†/è²»ç”¨ï¼‰', 'ç™ºè¡Œï¼ˆè‡ªç¤¾ãŒå—ã‘å–ã‚‹/å£²ä¸Šï¼‰'], key='stance_radio')
    stance_value = 'received' if stance.startswith('å—é ˜') else 'issued'
    st.session_state.current_stance = stance_value

    # æ¶ˆè²»ç¨åŒºåˆ†é¸æŠUI
    st_tax_mode = st.selectbox('æ¶ˆè²»ç¨åŒºåˆ†ï¼ˆè‡ªå‹•/å†…ç¨/å¤–ç¨/ç¨ç‡/éèª²ç¨ï¼‰', ['è‡ªå‹•åˆ¤å®š', 'å†…ç¨10%', 'å¤–ç¨10%', 'å†…ç¨8%', 'å¤–ç¨8%', 'éèª²ç¨'], key='tax_mode_select')
    st.session_state.current_tax_mode = st_tax_mode

    # PDFç”»åƒåŒ–OCRå¼·åˆ¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    force_pdf_ocr = st.checkbox('PDFã¯å¸¸ã«ç”»åƒåŒ–ã—ã¦OCRã™ã‚‹ï¼ˆæ¨å¥¨ï¼šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå´©ã‚Œã‚„ãƒ•ãƒƒã‚¿ãƒ¼èª¤èªè­˜å¯¾ç­–ï¼‰', value=False, key='force_pdf_ocr_checkbox')
    st.session_state.force_pdf_ocr = force_pdf_ocr

    output_mode = st.selectbox('å‡ºåŠ›å½¢å¼ã‚’é¸æŠ', ['æ±ç”¨CSV', 'æ±ç”¨TXT', 'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰CSV', 'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰TXT'], key='output_mode_select')
    st.session_state.current_output_mode = output_mode

    uploaded_files = st.file_uploader('ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰\nâ€»HEICã¯æœªå¯¾å¿œã€‚JPEG/PNG/PDFã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„', type=['png', 'jpg', 'jpeg', 'pdf'], accept_multiple_files=True, key='file_uploader')

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®å‡¦ç†
    if uploaded_files:
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆã®ã¿å‡¦ç†
        current_files = [(f.name, f.getvalue()) for f in uploaded_files]
        if current_files != st.session_state.uploaded_files_data:
            st.session_state.uploaded_files_data = current_files
            st.session_state.processed_results = []  # çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.csv_file_info = None  # CSVãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆ
            
            for uploaded_file in uploaded_files:
                file_path = os.path.join('input', uploaded_file.name)
                with open(file_path, 'wb') as f:
                    f.write(uploaded_file.getbuffer())
            st.success(f'{len(uploaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚')

    # å˜ä¸€å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    extra_prompt = st.text_area('AIã¸ã®è¿½åŠ æŒ‡ç¤ºãƒ»ãƒ’ãƒ³ãƒˆ', '', key='extra_prompt_textarea')
    
    # ä»•è¨³CSVä½œæˆãƒœã‚¿ãƒ³
    if st.button('ä»•è¨³CSVã‚’ä½œæˆ', type='primary', key='create_csv_button'):
        with st.spinner('ä»•è¨³å‡¦ç†ä¸­...'):
            all_entries = []
            for uploaded_file in uploaded_files:
                file_path = os.path.join('input', uploaded_file.name)
                
                # OCRå‡¦ç†
                if uploaded_file.name.lower().endswith('.pdf'):
                    if st.session_state.get('force_pdf_ocr', False):
                        # PDFã‚’ç”»åƒåŒ–ã—ã¦OCR
                        try:
                            with open(file_path, 'rb') as f:
                                pdf_content = f.read()
                            images = pdf_to_images_pdfco(pdf_content, PDFCO_API_KEY)
                            text = ""
                            for img_content in images:
                                img_temp_path = os.path.join('input', f'temp_img_{int(time.time())}.jpg')
                                with open(img_temp_path, 'wb') as f:
                                    f.write(img_content)
                                text += ocr_image(img_temp_path, mode='gcv') + "\n"
                                os.remove(img_temp_path)
                        except Exception as e:
                            st.warning(f"PDFç”»åƒåŒ–OCRã«å¤±æ•—: {e}")
                            text = extract_text_from_pdf(uploaded_file.getvalue())
                    else:
                        text = extract_text_from_pdf(uploaded_file.getvalue())
                else:
                    text = ocr_image(file_path, mode='gcv')
                
                # ãƒ†ã‚­ã‚¹ãƒˆãŒååˆ†ã‹ãƒã‚§ãƒƒã‚¯
                if not is_text_sufficient(text):
                    st.warning(f'{uploaded_file.name}: ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ååˆ†ã§ã™')
                    continue
                
                # ä»•è¨³æƒ…å ±æŠ½å‡º
                entries = extract_multiple_entries(text, stance_value, st_tax_mode, debug_mode, extra_prompt)
                all_entries.extend(entries)
            
            # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state.processed_results = all_entries
            
            # CSVç”Ÿæˆ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'journal_{timestamp}'
            
            mode_map = {
                'æ±ç”¨CSV': 'default',
                'æ±ç”¨TXT': 'default',
                'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰CSV': 'mf',
                'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰TXT': 'mf'
            }
            
            as_txt = output_mode.endswith('TXT')
            csv_result = generate_csv(all_entries, filename, mode_map[output_mode], as_txt)
            
            if csv_result:
                st.session_state.csv_file_info = csv_result
                st.success(f'âœ… {len(all_entries)}ä»¶ã®ä»•è¨³ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼')
                st.rerun()
    
    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    if 'csv_file_info' in st.session_state and st.session_state.csv_file_info:
        try:
            csv_info = st.session_state.csv_file_info
            if isinstance(csv_info, dict) and 'path' in csv_info and 'filename' in csv_info:
                with open(csv_info['path'], 'rb') as f:
                    st.download_button(
                        f"ğŸ“¥ {csv_info['filename']} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        f,
                        file_name=csv_info['filename'],
                        mime=csv_info.get('mime_type', 'text/csv')
                    )
        except Exception as e:
            st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            if 'csv_file_info' in st.session_state:
                del st.session_state.csv_file_info

    # å‡¦ç†æ¸ˆã¿çµæœãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
    if st.session_state.processed_results:
        st.write("### ğŸ“‹ å‡¦ç†æ¸ˆã¿ã®ä»•è¨³çµæœ")
        st.success("âœ… ä»•è¨³å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ä»¥ä¸‹ã®çµæœã‚’ç¢ºèªã—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
        
        for i, result in enumerate(st.session_state.processed_results):
            st.write(f"**ğŸ“„ ä»•è¨³ {i+1}:**")
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"ğŸ¢ **ä¼šç¤¾å:** {result['company']}")
                st.write(f"ğŸ“… **æ—¥ä»˜:** {result['date']}")
                st.write(f"ğŸ’° **é‡‘é¡:** {result['amount']}")
            with col2:
                st.write(f"ğŸ§¾ **æ¶ˆè²»ç¨:** {result['tax']}")
                st.write(f"ğŸ“ **æ‘˜è¦:** {result['description']}")
                st.write(f"ğŸ·ï¸ **å‹˜å®šç§‘ç›®:** {result['account']}")
            st.write(f"ğŸ¤– **æ¨æ¸¬æ–¹æ³•:** {result['account_source']}")
            
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ã‚’è¿½åŠ 
            st.write("---")
            st.subheader(f"ä»•è¨³ {i+1} ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
            review_key = f"review_state_{i}"
            if review_key not in st.session_state:
                st.session_state[review_key] = "æ­£ã—ã„"
            
            reviewer_name = st.text_input("ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‹…å½“è€…å", key=f"reviewer_{i}")
            
            # ç¾åœ¨ã®é¸æŠçŠ¶æ…‹ã‚’è¡¨ç¤º
            st.write(f"**ç¾åœ¨ã®é¸æŠ: {st.session_state[review_key]}**")
            
            # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®ä»£ã‚ã‚Šã«ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨
            col1, col2 = st.columns(2)
            with col1:
                if st.button("âœ… æ­£ã—ã„", key=f"correct_btn_{i}", type="primary" if st.session_state[review_key] == "æ­£ã—ã„" else "secondary"):
                    st.session_state[review_key] = "æ­£ã—ã„"
                    st.rerun()
            with col2:
                if st.button("âŒ ä¿®æ­£ãŒå¿…è¦", key=f"incorrect_btn_{i}", type="primary" if st.session_state[review_key] == "ä¿®æ­£ãŒå¿…è¦" else "secondary"):
                    st.session_state[review_key] = "ä¿®æ­£ãŒå¿…è¦"
                    st.rerun()
            
            # æ¡ä»¶åˆ†å²ã‚’åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†é›¢
            if st.session_state[review_key] == "ä¿®æ­£ãŒå¿…è¦":
                st.write("**ä¿®æ­£å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š**")
                corrected_account = st.text_input("ä¿®æ­£å¾Œã®å‹˜å®šç§‘ç›®", value=result['account'], key=f"account_{i}")
                corrected_description = st.text_input("ä¿®æ­£å¾Œã®æ‘˜è¦", value=result['description'], key=f"desc_{i}")
                comments = st.text_area("ä¿®æ­£ç†ç”±ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆ", placeholder="ä¿®æ­£ãŒå¿…è¦ãªç†ç”±ã‚„è¿½åŠ ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key=f"comments_{i}")
                
                # ä¿®æ­£å†…å®¹ã‚’ä¿å­˜ãƒœã‚¿ãƒ³
                if st.button("ğŸ’¾ ä¿®æ­£å†…å®¹ã‚’ä¿å­˜", key=f"save_corrected_{i}", type="primary"):
                    # ä¿®æ­£å¾Œã®ä»•è¨³ã‚’ä½œæˆ
                    corrected_journal = f"ä»•è¨³: {corrected_account} {result['amount']}å††"
                    if result['tax'] != '0':
                        corrected_journal += f" (æ¶ˆè²»ç¨: {result['tax']}å††)"
                    corrected_journal += f" - {corrected_description}"
                    
                    # å…ƒã®ä»•è¨³ã‚’ä½œæˆ
                    original_journal = f"ä»•è¨³: {result['account']} {result['amount']}å††"
                    if result['tax'] != '0':
                        original_journal += f" (æ¶ˆè²»ç¨: {result['tax']}å††)"
                    original_journal += f" - {result['description']}"
                    
                    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¿å­˜
                    if save_review_to_firestore(
                        result.get('original_text', ''),
                        original_journal,
                        corrected_journal,
                        reviewer_name,
                        comments
                    ):
                        st.success("âœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                        cache_key = 'learning_data_cache'
                        cache_timestamp_key = 'learning_data_timestamp'
                        if cache_key in st.session_state:
                            del st.session_state[cache_key]
                        if cache_timestamp_key in st.session_state:
                            del st.session_state[cache_timestamp_key]
                        st.rerun()
                    else:
                        st.error("âŒ ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            elif st.session_state[review_key] == "æ­£ã—ã„":
                # æ­£ã—ã„ã¨ã—ã¦ä¿å­˜ãƒœã‚¿ãƒ³
                if st.button("âœ… æ­£ã—ã„ã¨ã—ã¦ä¿å­˜", key=f"save_correct_{i}", type="primary"):
                    # æ­£ã—ã„ä»•è¨³ã‚’ä½œæˆ
                    correct_journal = f"ä»•è¨³: {result['account']} {result['amount']}å††"
                    if result['tax'] != '0':
                        correct_journal += f" (æ¶ˆè²»ç¨: {result['tax']}å††)"
                    correct_journal += f" - {result['description']}"
                    
                    # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ä»•è¨³æƒ…å ±ã‹ã‚‰å†æ§‹ç¯‰
                    original_text = result.get('original_text', '')
                    if not original_text:
                        original_text = f"å–å¼•å…ˆ: {result.get('company', 'N/A')}, æ—¥ä»˜: {result.get('date', 'N/A')}, é‡‘é¡: {result.get('amount', 'N/A')}å††, æ‘˜è¦: {result.get('description', 'N/A')}"
                    
                    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¿å­˜ï¼ˆä¿®æ­£ãªã—ï¼‰
                    if save_review_to_firestore(
                        original_text,
                        correct_journal,
                        correct_journal,  # ä¿®æ­£ãªã—ãªã®ã§åŒã˜
                        reviewer_name,
                        "æ­£ã—ã„ä»•è¨³ã¨ã—ã¦ç¢ºèª"
                    ):
                        st.success("âœ… æ­£ã—ã„ä»•è¨³ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸï¼")
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                        cache_key = 'learning_data_cache'
                        cache_timestamp_key = 'learning_data_timestamp'
                        if cache_key in st.session_state:
                            del st.session_state[cache_key]
                        if cache_timestamp_key in st.session_state:
                            del st.session_state[cache_timestamp_key]
                        st.rerun()
                    else:
                        st.error("âŒ ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

with tab2:
    st.subheader("ğŸš€ ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
    
    # ãƒãƒƒãƒå‡¦ç†ã®UI
    st.write("è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†ã§ãã¾ã™ã€‚")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = st.file_uploader(
        "è¤‡æ•°ã®ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['png', 'jpg', 'jpeg', 'pdf'],
        accept_multiple_files=True,
        help="è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )
    
    if uploaded_files:
        st.write(f"ğŸ“ {len(uploaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
        
        # å‡¦ç†è¨­å®š
        col1, col2 = st.columns(2)
        with col1:
            batch_stance = st.radio(
                "ã“ã®è«‹æ±‚æ›¸ã¯ã©ã¡ã‚‰ã®ç«‹å ´ã§ã™ã‹?",
                ["å—é ˜ (è‡ªç¤¾ãŒæ”¯æ‰•ã†/è²»ç”¨)", "ç™ºè¡Œ (è‡ªç¤¾ãŒå—ã‘å–ã‚‹/å£²ä¸Š)"],
                key="batch_stance"
            )
        
        with col2:
            batch_tax_mode = st.selectbox(
                "æ¶ˆè²»ç¨åŒºåˆ†",
                ["è‡ªå‹•åˆ¤å®š", "å†…ç¨", "å¤–ç¨", "éèª²ç¨"],
                key="batch_tax_mode"
            )
        
        batch_output_format = st.selectbox(
            "å‡ºåŠ›å½¢å¼ã‚’é¸æŠ",
            ["æ±ç”¨CSV", "æ±ç”¨TXT", "ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰CSV", "ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰TXT"],
            key="batch_output_format"
        )
        
        batch_extra_prompt = st.text_area(
            "AIã¸ã®è¿½åŠ æŒ‡ç¤ºãƒ»ãƒ’ãƒ³ãƒˆ",
            placeholder="ä¾‹: ã“ã®ä¼šç¤¾ã®ä»•è¨³ã¯é€šå¸¸ã€é€šä¿¡è²»ã¨ã—ã¦å‡¦ç†ã—ã¾ã™",
            key="batch_extra_prompt"
        )
        
        # å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
        if st.button("ğŸš€ ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹", type="primary"):
            if uploaded_files:
                # ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
                st.write("ğŸ”„ ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                
                all_results = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, uploaded_file in enumerate(uploaded_files):
                    status_text.text(f"å‡¦ç†ä¸­: {uploaded_file.name} ({i+1}/{len(uploaded_files)})")
                    
                    try:
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                        file_content = uploaded_file.read()
                        uploaded_file.seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                        if uploaded_file.type == "application/pdf":
                            # PDFå‡¦ç†
                            text = extract_text_from_pdf(file_content)
                        else:
                            # ç”»åƒå‡¦ç†
                            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                                tmp_file.write(file_content)
                                tmp_file.flush()
                                text = ocr_image_gcv(tmp_file.name)
                                os.unlink(tmp_file.name)
                        
                        if text and is_text_sufficient(text):
                            # ä»•è¨³æƒ…å ±ã‚’æŠ½å‡º
                            stance_value = 'received' if 'å—é ˜' in batch_stance else 'issued'
                            results = extract_multiple_entries(text, stance_value, batch_tax_mode, False, batch_extra_prompt)
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
                            for result in results:
                                result['filename'] = uploaded_file.name
                            
                            all_results.extend(results)
                            st.success(f"âœ… {uploaded_file.name}: {len(results)}ä»¶ã®ä»•è¨³ã‚’æŠ½å‡º")
                        else:
                            st.warning(f"âš ï¸ {uploaded_file.name}: ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ååˆ†ã§ã™")
                            
                    except Exception as e:
                        st.error(f"âŒ {uploaded_file.name}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {str(e)}")
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                status_text.text("å‡¦ç†å®Œäº†ï¼")
                
                if all_results:
                    # çµæœã‚’è¡¨ç¤º
                    st.write(f"ğŸ“Š åˆè¨ˆ {len(all_results)}ä»¶ã®ä»•è¨³ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                    
                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"batch_processing_{timestamp}"
                    
                    # å‡ºåŠ›å½¢å¼ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
                    if "CSV" in batch_output_format:
                        csv_result = generate_csv(all_results, filename, 
                                              'mf' if 'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰' in batch_output_format else 'default', 
                                              False)
                        with open(csv_result['path'], 'rb') as f:
                            csv_data = f.read()
                        st.download_button(
                            label="ğŸ“¥ ãƒãƒƒãƒå‡¦ç†çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV)",
                            data=csv_data,
                            file_name=csv_result['filename'],
                            mime=csv_result['mime_type']
                        )
                    else:
                        txt_result = generate_csv(all_results, filename, 
                                              'mf' if 'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰' in batch_output_format else 'default', 
                                              True)
                        with open(txt_result['path'], 'rb') as f:
                            txt_data = f.read()
                        st.download_button(
                            label="ğŸ“¥ ãƒãƒƒãƒå‡¦ç†çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (TXT)",
                            data=txt_data,
                            file_name=txt_result['filename'],
                            mime=txt_result['mime_type']
                        )
                    
                    # çµæœã®è©³ç´°è¡¨ç¤º
                    with st.expander("ğŸ“‹ å‡¦ç†çµæœã®è©³ç´°"):
                        for result in all_results:
                            st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«: {result['filename']}**")
                            st.write(f"å–å¼•å…ˆ: {result.get('company', 'N/A')}")
                            st.write(f"é‡‘é¡: {result.get('amount', 'N/A')}")
                            st.write(f"å‹˜å®šç§‘ç›®: {result.get('account', 'N/A')}")
                            st.write("---")
                else:
                    st.error("âŒ å‡¦ç†å¯èƒ½ãªä»•è¨³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.error("ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    else:
        st.info("ğŸ“ è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¦ãã ã•ã„")

def process_batch_files(uploaded_files, stance, tax_mode, output_format, extra_prompt):
    """ãƒãƒƒãƒå‡¦ç†ã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    st.write("ğŸ”„ ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    
    all_results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"å‡¦ç†ä¸­: {uploaded_file.name} ({i+1}/{len(uploaded_files)})")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            file_content = uploaded_file.read()
            uploaded_file.seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            if uploaded_file.type == "application/pdf":
                # PDFå‡¦ç†
                text = extract_text_from_pdf(file_content)
            else:
                # ç”»åƒå‡¦ç†
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                    tmp_file.write(file_content)
                    tmp_file.flush()
                    text = ocr_image_gcv(tmp_file.name)
                    os.unlink(tmp_file.name)
            
            if text and is_text_sufficient(text):
                # ä»•è¨³æƒ…å ±ã‚’æŠ½å‡º
                stance_value = 'received' if 'å—é ˜' in stance else 'issued'
                results = extract_multiple_entries(text, stance_value, tax_mode, False, extra_prompt)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
                for result in results:
                    result['filename'] = uploaded_file.name
                
                all_results.extend(results)
                st.success(f"âœ… {uploaded_file.name}: {len(results)}ä»¶ã®ä»•è¨³ã‚’æŠ½å‡º")
            else:
                st.warning(f"âš ï¸ {uploaded_file.name}: ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ååˆ†ã§ã™")
                
        except Exception as e:
            st.error(f"âŒ {uploaded_file.name}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {str(e)}")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
        progress_bar.progress((i + 1) / len(uploaded_files))
    
    status_text.text("å‡¦ç†å®Œäº†ï¼")
    
    if all_results:
        # çµæœã‚’è¡¨ç¤º
        st.write(f"ğŸ“Š åˆè¨ˆ {len(all_results)}ä»¶ã®ä»•è¨³ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_processing_{timestamp}"
        
        # å‡ºåŠ›å½¢å¼ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        if "CSV" in output_format:
            csv_result = generate_csv(all_results, filename, 
                                  'mf' if 'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰' in output_format else 'default', 
                                  False)
            with open(csv_result['path'], 'rb') as f:
                csv_data = f.read()
            st.download_button(
                label="ğŸ“¥ ãƒãƒƒãƒå‡¦ç†çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV)",
                data=csv_data,
                file_name=csv_result['filename'],
                mime=csv_result['mime_type']
            )
        else:
            txt_result = generate_csv(all_results, filename, 
                                  'mf' if 'ãƒãƒãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰' in output_format else 'default', 
                                  True)
            with open(txt_result['path'], 'rb') as f:
                txt_data = f.read()
            st.download_button(
                label="ğŸ“¥ ãƒãƒƒãƒå‡¦ç†çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (TXT)",
                data=txt_data,
                file_name=txt_result['filename'],
                mime=txt_result['mime_type']
            )
        
        # çµæœã®è©³ç´°è¡¨ç¤º
        with st.expander("ğŸ“‹ å‡¦ç†çµæœã®è©³ç´°"):
            for result in all_results:
                st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«: {result['filename']}**")
                st.write(f"å–å¼•å…ˆ: {result.get('company', 'N/A')}")
                st.write(f"é‡‘é¡: {result.get('amount', 'N/A')}")
                st.write(f"å‹˜å®šç§‘ç›®: {result.get('account', 'N/A')}")
                st.write("---")
    else:
        st.error("âŒ å‡¦ç†å¯èƒ½ãªä»•è¨³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def batch_processing_ui():
    """ãƒãƒƒãƒå‡¦ç†UIã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼é–¢æ•°"""
    st.info("ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½ã¯ç¾åœ¨é–‹ç™ºä¸­ã§ã™ã€‚")

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…
def initialize_vector_model():
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    if not VECTOR_SEARCH_AVAILABLE:
        return None
    
    try:
        # æ—¥æœ¬èªå¯¾å¿œã®Sentence Transformerãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        return model
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def create_text_embeddings(texts, model):
    """ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
    if not VECTOR_SEARCH_AVAILABLE or model is None:
        return None
    
    try:
        embeddings = model.encode(texts, show_progress_bar=False)
        return embeddings
    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def build_vector_index(reviews, model):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
    if not VECTOR_SEARCH_AVAILABLE or model is None:
        return None
    
    try:
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
        texts = []
        for review in reviews:
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã€AIä»•è¨³ã€ä¿®æ­£å¾Œä»•è¨³ã‚’çµåˆ
            text_parts = []
            if review.get('original_text'):
                text_parts.append(review['original_text'])
            if review.get('ai_journal'):
                text_parts.append(review['ai_journal'])
            if review.get('corrected_journal'):
                text_parts.append(review['corrected_journal'])
            if review.get('comments'):
                text_parts.append(review['comments'])
            
            combined_text = ' '.join(text_parts)
            texts.append(combined_text)
        
        if not texts:
            return None
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embeddings = create_text_embeddings(texts, model)
        if embeddings is None:
            return None
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
        
        # æ­£è¦åŒ–ã—ã¦cosine similarityã‚’è¨ˆç®—
        faiss.normalize_L2(embeddings)
        index.add(embeddings.astype('float32'))
        
        return {
            'index': index,
            'reviews': reviews,
            'texts': texts,
            'embeddings': embeddings
        }
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def search_similar_reviews_vector(query_text, vector_index, model, top_k=5, similarity_threshold=0.3):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹é¡ä¼¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ¤œç´¢"""
    if not VECTOR_SEARCH_AVAILABLE or vector_index is None or model is None:
        return []
    
    try:
        # ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_embedding = model.encode([query_text], show_progress_bar=False)
        faiss.normalize_L2(query_embedding)
        
        # é¡ä¼¼åº¦æ¤œç´¢
        similarities, indices = vector_index['index'].search(
            query_embedding.astype('float32'), 
            min(top_k, len(vector_index['reviews']))
        )
        
        # çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if similarity >= similarity_threshold:
                review = vector_index['reviews'][idx]
                results.append({
                    'review': review,
                    'similarity': float(similarity),
                    'rank': i + 1
                })
        
        return results
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def get_vector_search_status():
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ç¢ºèª"""
    if not VECTOR_SEARCH_AVAILABLE:
        return {
            'available': False,
            'message': 'ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“',
            'recommendation': 'sentence-transformersã€scikit-learnã€faiss-cpuã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„'
        }
    
    model = initialize_vector_model()
    if model is None:
        return {
            'available': False,
            'message': 'ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ',
            'recommendation': 'ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„'
        }
    
    return {
        'available': True,
        'message': 'ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã§ã™',
        'model': model
    }

def hybrid_search_similar_reviews(text, reviews, vector_model=None, top_k=5):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆçµ±è¨ˆçš„æ¤œç´¢ + ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼‰"""
    results = []
    
    # 1. çµ±è¨ˆçš„æ¤œç´¢ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
    statistical_results = find_similar_reviews_advanced(text, reviews)
    
    # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    vector_results = []
    if VECTOR_SEARCH_AVAILABLE and vector_model is not None:
        vector_index = build_vector_index(reviews, vector_model)
        if vector_index is not None:
            vector_results = search_similar_reviews_vector(text, vector_index, vector_model, top_k)
    
    # 3. çµæœã®çµ±åˆã¨é‡è¤‡é™¤å»
    seen_review_ids = set()
    
    # çµ±è¨ˆçš„æ¤œç´¢çµæœã‚’è¿½åŠ 
    for result in statistical_results:
        review_id = result.get('doc_id', '')
        if review_id not in seen_review_ids:
            results.append({
                'review': result,
                'similarity': result.get('similarity', 0.0),
                'search_method': 'statistical',
                'rank': len(results) + 1
            })
            seen_review_ids.add(review_id)
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’è¿½åŠ 
    for result in vector_results:
        review_id = result['review'].get('doc_id', '')
        if review_id not in seen_review_ids:
            results.append({
                'review': result['review'],
                'similarity': result['similarity'],
                'search_method': 'vector',
                'rank': len(results) + 1
            })
            seen_review_ids.add(review_id)
    
    # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['similarity'], reverse=True)
    
    return results[:top_k]

def generate_hybrid_learning_prompt(text, similar_reviews):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢çµæœã‹ã‚‰å­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    if not similar_reviews:
        return ""
    
    prompt_parts = []
    prompt_parts.append("ã€éå»ã®é¡ä¼¼äº‹ä¾‹ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + çµ±è¨ˆçš„æ¤œç´¢ï¼‰ã€‘")
    
    for i, result in enumerate(similar_reviews):
        review = result['review']
        similarity = result['similarity']
        search_method = result.get('search_method', 'unknown')
        
        # æ¤œç´¢æ–¹æ³•ã®ã‚¢ã‚¤ã‚³ãƒ³
        method_icon = "ğŸš€" if search_method == 'vector' else "ğŸ“Š"
        
        prompt_parts.append(f"\n{method_icon} é¡ä¼¼åº¦ {similarity:.2f} - äº‹ä¾‹ {i+1}:")
        
        # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆçŸ­ç¸®ç‰ˆï¼‰
        original_text = review.get('original_text', '')
        if len(original_text) > 100:
            original_text = original_text[:100] + "..."
        prompt_parts.append(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {original_text}")
        
        # AIæ¨æ¸¬ã¨ä¿®æ­£
        ai_journal = review.get('ai_journal', '')
        corrected_journal = review.get('corrected_journal', '')
        
        if ai_journal and corrected_journal:
            if ai_journal != corrected_journal:
                prompt_parts.append(f"AIæ¨æ¸¬: {ai_journal}")
                prompt_parts.append(f"ä¿®æ­£å¾Œ: {corrected_journal}")
                prompt_parts.append("â†’ ä¿®æ­£ãŒå¿…è¦ã§ã—ãŸ")
            else:
                prompt_parts.append(f"ä»•è¨³: {ai_journal}")
                prompt_parts.append("â†’ æ­£ã—ã„ä»•è¨³ã§ã—ãŸ")
        
        # ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Œã°è¿½åŠ 
        comments = review.get('comments', '')
        if comments:
            prompt_parts.append(f"ã‚³ãƒ¡ãƒ³ãƒˆ: {comments}")
    
    prompt_parts.append("\nã€å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆã€‘")
    prompt_parts.append("ä¸Šè¨˜ã®é¡ä¼¼äº‹ä¾‹ã‚’å‚è€ƒã«ã€åŒã˜ã‚ˆã†ãªé–“é•ã„ã‚’é¿ã‘ã¦æ­£ç¢ºãªå‹˜å®šç§‘ç›®ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    
    return "\n".join(prompt_parts)
